{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"Lista11.ipynb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Automatically generated by Colaboratory."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Original file is located at\n", "    https://colab.research.google.com/drive/1L2k4Mqp10rClJXig4cfwda3OVClYoMZh"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Aula 6 - Regress\u00e3o Log\u00edstica"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "# -*- coding: utf8<br>\n", "import numpy as np<br>\n", "import matplotlib.pyplot as plt<br>\n", "from mpl_toolkits.mplot3d import Axes3D<br>\n", "import pandas as pd<br>\n", "from numpy.testing import assert_almost_equal<br>\n", "from numpy.testing import assert_equal<br>\n", "from numpy.testing import assert_array_almost_equal<br>\n", "from numpy.testing import assert_array_equal<br>\n", "# Breast Cancer Dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Esse dataset cont\u00e9m dados de pacientes com tumores de mama, voc\u00ea tentar\u00e1 prever se um tumor \u00e9 maligno ou benigno de acordo com a espessura do tumor. No gr\u00e1fico abaixo voc\u00ea pode visualizar os tipos de tumores (1 = maligno, 0 = benigno) por categoria de espessura."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["|Index | Attribute | Domain |\n", "|----|----|----|\n", "0|Sample code number | id number\n", "1|Clump Thickness | 1 - 10\n", "2|Uniformity of Cell Size | 1 - 10\n", "3|Uniformity of Cell Shape | 1 - 10\n", "4|Marginal Adhesion | 1 - 10\n", "5|Single Epithelial Cell Size | 1 - 10\n", "6|Bare Nuclei | 1 - 10\n", "7|Bland Chromatin | 1 - 10\n", "8|Normal Nucleoli | 1 - 10\n", "9|Mitoses | 1 - 10\n", "10|Class | (2 for benign, 4 for malignant)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pr\u00e9-processamento dos dados"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/'\\<br>\n", "                'breast-cancer-wisconsin/breast-cancer-wisconsin.data',<br>\n", "                header=None)<br>\n", "# o nosso dataset cont\u00e9m strings `?` no lugar de dados faltantes<br>\n", "# vamos substituir esses valores para n\u00e3o termos problemas na execu\u00e7\u00e3o<br>\n", "# das abordagens num\u00e9ricas<br>\n", "df = df.replace('?', np.NaN)<br>\n", "scaled_df = df.iloc[:, 1:-1].copy()<br>\n", "scaled_df = scaled_df.drop(columns=6)<br>\n", "scaled_df = (scaled_df - scaled_df.mean())/scaled_df.std()<br>\n", "X = scaled_df<br>\n", "X = np.nan_to_num(X)<br>\n", "y = df.iloc[:, -1]<br>\n", "y = y.values<br>\n", "y[y==2] = 0<br>\n", "y[y==4] = 1<br>\n", "nfim, vamos obter os conjuntos de treino e teste para os algoritmos que vamos implementar:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \\\n", "                                                    random_state=32)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Regress\u00e3o Log\u00edstica<br>\n", "Como discutido na aula, a regress\u00e3o log\u00edstica baseia-se na fun\u00e7\u00e3o sigmoide:<br>\n", "$$f(x) = \\frac{1}{1+e^{x}}$$<br>\n", "\u00c0 medida que sua entrada se torna grande e positiva, $f(x)$ se aproxima e se aproxima de 1. \u00c0 medida que sua entrada se torna grande e negativa, $f(x)$ se aproxima e se aproxima de 0.<br>\n", "### Fun\u00e7\u00e3o Sigmoide<br>\n", "Voc\u00ea consegue implementar a fun\u00e7\u00e3o `sigmoid` abaixo?<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sigmoid(x):\n", "    # Seu c\u00f3digo aqui\n", "    return 1 / (1 + np.exp(-x))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nAl\u00e9m disso, $f(x)$ tem a propriedade conveniente que sua derivada \u00e9 dada por:<br>\n", "$$f'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = f(x)(1-f(x))$$<br>\n", "Vamos testar a fun\u00e7\u00e3o implementada:<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = np.linspace(-5,5,100)\n", "y = sigmoid(x)\n", "plt.plot(x, y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n### Derivada da Sigmoide<br>\n", "Voc\u00ea consegue implementar a fun\u00e7\u00e3o `sigmoid_prime` abaixo?<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sigmoid_prime(x):\n", "    # Seu c\u00f3digo aqui\n", "    return sigmoid(x) * (1 - sigmoid(x))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x, sigmoid_prime(x))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nConsidere que possu\u00edmos um conjunto $X_{(n \\times k)}$ de $n$ dados - cada dado com $k$ atributos (_features_) - e suas respectivas classes $\\mathrm{y}$ - cada classe \u00e9 zero ou um.<br>\n", "Um par $(\\mathbf{x_i}, y_i)$ \u00e9 um exemplo do nosso conjunto, onde $\\mathbf{x_i}$ \u00e9 a i-\u00e9sima linha de $X$ e $y_i$ \u00e9 sua respectiva classe.<br>\n", "Vamos usar a fun\u00e7\u00e3o sigmoide $f$ para ajustar um modelo de log\u00edstico linear:<br>\n", "$$y_i = f(\\mathbf{x_i^T}\\boldsymbol{\\theta}) + \\epsilon_i = f_{\\boldsymbol{\\theta}}(\\mathbf{x_i}) + \\epsilon_i$$<br>\n", "Note que $\\mathbf{x_i^T}\\boldsymbol{\\theta}$, para $k$ vari\u00e1veis independentes, nada mais \u00e9 que o modelo linear visto nas aulas anteriores, que \u00e9 calculado e dado como entrada para a fun\u00e7\u00e3o log\u00edstica:<br>\n", "$$\\mathbf{x_i^T}\\boldsymbol{\\theta} = \\theta_0 + \\theta_1 x_{i1} + \\cdots + \\theta_k x_{ik}$$<br>\n", "Ao contr\u00e1rio do que acontece no caso da Regress\u00e3o Linear, precisamos calcular diretamente a fun\u00e7\u00e3o de verossimilhan\u00e7a e seu gradiente. Por\u00e9m, vimos que \u00e9 mais simples maximizar o logaritmo da verossimilhan\u00e7a (*log likelihood*):<br>\n", "$$\\log ll_{\\theta}(y_i~|~\\mathbf{x_i}) = y_i \\log f_{\\boldsymbol{\\theta}}(\\mathbf{x_i}) + (1-y_i) \\log (1-f_{\\boldsymbol{\\theta}}(\\mathbf{x_i}))$$<br>\n", "### Cross Entropy<br>\n", "Ao inv\u00e9s de trabalhar na verossimilhan\u00e7a, vamos invert\u00ea-la e utilizar o *cross entropy* para a regress\u00e3o log\u00edstica.<br>\n", "$$L(\\boldsymbol{\\theta}) = -n^{-1}\\sum_i \\big((1-y_i)\\log_2(1-f_{\\boldsymbol{\\theta}}(\\mathbf x_i)) + y_i\\log_2(f_{\\boldsymbol{\\theta}}(\\mathbf x_i))\\big)$$<br>\n", "A equa\u00e7\u00e3o acima \u00e9 a cross-entropy m\u00e9dia por observa\u00e7\u00e3o.<br>\n", "#### M\u00e9dia da Cross Entropy<br>\n", "Voc\u00ea consegue implementar a cross-entropy m\u00e9dia na fun\u00e7\u00e3o `cross_entropy_mean` abaixo?<br>\n", "Dica: voc\u00ea pode utilizar a fun\u00e7\u00e3o `np.clip(vetor, limite_inferior, limite_superior)` para evitar imprecis\u00f5es num\u00e9ricas quando calcular logar\u00edtimos ou realizar divis\u00f5es.<br>\n", "Nesse caso, voc\u00ea pode usar o `np.clip` para limitar o resultado da fun\u00e7\u00e3o `sigmoid` entre 0.0001 e 0.9999. Assim, se o vetor tiver um valor 1.01 por erro num\u00e9rico, corrigimos para 0.9999.<br>\n", "```<br>\n", "resultado = sigmoid(vetor)<br>\n", "novo_resultado = np.clip(resultado, 0.00001, 0.99999)<br>\n", "```<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cross_entropy_mean(X, y, theta):\n", "    z = np.dot(X, theta)\n", "    h = sigmoid(z)\n", "    h = np.clip(h, 0.00001, 0.99999)\n", "    cross_entropy = -(y * np.log(h) + (1 - y) * np.log(1 - h))\n", "    return cross_entropy.mean()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n### Derivada da Cross Entropy M\u00e9dia$<br>\n", "A derivada de $L(\\theta)$ tem uma forma similar ao da regress\u00e3o linear. A deriva\u00e7\u00e3o \u00e9 similar a que foi apresentada nos [slides](https://docs.google.com/presentation/d/1yGPETPe8o7PPOP6_CF38LHr3vpxgTEnF5LjP-1pkGIc/edit?usp=sharing).<br>\n", "\\begin{align*}<br>\n", "  \\frac{\\partial L(y_i, \\mathbf{x_i}, \\boldsymbol{\\theta})}{\\partial \\theta_j}<br>\n", "    &=  -\\frac{1}{n}\\sum_i<br>\n", "      \\left(<br>\n", "          (1-y_i)\\frac{\\partial}{\\partial \\theta_j}\\log_2(1-f_{\\boldsymbol{\\theta}}(\\mathbf x_i)) + y_i\\frac{\\partial}{\\partial \\theta_j}\\log_2(f_{\\boldsymbol{\\theta}}(\\mathbf x_i))<br>\n", "      \\right)\\\\<br>\n", "    &= -\\frac{1}{n}\\sum_i<br>\n", "      \\left(<br>\n", "          -\\frac{(1-y_i)x_{ij}f'(\\mathbf{x_i}\\boldsymbol{\\theta})}{1- f(\\mathbf{x_i}\\boldsymbol{\\theta})} + \\frac{y_ix_{ij}f'(\\mathbf{x_i}\\boldsymbol{\\theta}) }{f(\\mathbf{x_i}\\boldsymbol{\\theta})}<br>\n", "      \\right)\\\\<br>\n", "    &= -\\frac{1}{n}\\sum_i<br>\n", "      \\left(<br>\n", "          -\\frac{(1-y_i)}{1- f(\\mathbf{x_i}\\boldsymbol{\\theta})} + \\frac{y_i}{f(\\mathbf{x_i}\\boldsymbol{\\theta})}<br>\n", "      \\right)<br>\n", "        x_{ij}f(\\mathbf{x_i}\\boldsymbol{\\theta})(1 - f(\\mathbf{x_i}\\boldsymbol{\\theta}))\\\\<br>\n", "    &= -\\frac{1}{n}\\sum_i<br>\n", "      \\left(<br>\n", "          \\frac{<br>\n", "            -(1-y_i)f(\\mathbf{x_i}\\boldsymbol{\\theta}) + y_i(1- f(\\mathbf{x_i}\\boldsymbol{\\theta}))<br>\n", "          }{<br>\n", "            f(\\mathbf{x_i}\\boldsymbol{\\theta})(1- f(\\mathbf{x_i}\\boldsymbol{\\theta}))<br>\n", "          }<br>\n", "      \\right)<br>\n", "        x_{ij}f(\\mathbf{x_i}\\boldsymbol{\\theta})(1 - f(\\mathbf{x_i}\\boldsymbol{\\theta}))\\\\<br>\n", "    &= -\\frac{1}{n}\\sum_i<br>\n", "      \\left(<br>\n", "            y_i - f(\\mathbf{x_i}\\boldsymbol{\\theta})<br>\n", "      \\right)x_{ij}\\\\<br>\n", "    &= -\\frac{1}{n}\\sum_i<br>\n", "      \\left(<br>\n", "            y_i - f_\\boldsymbol{\\theta}(\\mathbf{x_i})<br>\n", "      \\right)x_{ij}\\\\<br>\n", "    &= -\\frac{1}{n}X^T_j \\big(\\mathbf{y} - f_\\boldsymbol{\\theta}(X)\\big)<br>\n", "\\end{align*}<br>\n", "Logo:<br>\n", "\\begin{align*}<br>\n", "  \\frac{\\partial L(y_i, \\mathbf{x_i}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}<br>\n", "    &= -\\frac{1}{n}X^T \\big(\\mathbf{y} - f_\\boldsymbol{\\theta}(X)\\big)<br>\n", "\\end{align*}<br>\n", "#### Derivada Vetorizada<br>\n", "Voc\u00ea consegue escrever a forma vetorizada da derivada na fun\u00e7\u00e3o `derivadas` abaixo?<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def derivadas(X, y, theta):\n", "    # Seu c\u00f3digo aqui\n", "    z = np.dot(X, theta)\n", "    h = sigmoid(z)\n", "    error = h - y\n", "    grad = np.dot(X.T, error) / len(y)\n", "    return grad"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Gradiente Descendente<br>\n", "Na Aula 5, utilizamos o gradiente descendente no contexto da Regress\u00e3o Linear. Agora, vamos aplicar o gradiente descendente para o caso da Regress\u00e3o Log\u00edstica. A ideia aqui \u00e9 a mesma:<br>\n", "1. Escolha um valor inicial do vetor (e.g., \u03b8 = [0.2, 1, -2, 5])<br>\n", "2. Escolha uma taxa de aprendizado (e.g., \u03bb = 0.01)<br>\n", "3. Repita:<br>\n", "  1. Compute a derivada da log-verossimilhan\u00e7a (log likelihood) ll'(\u03b8)<br>\n", "  2. Atualize $\\theta$<br>\n", "4. Pare quando convergir<br>\n", "#### Exerc\u00edcio 1<br>\n", "Modifique a implementa\u00e7\u00e3o de `gd` para o caso da regress\u00e3o log\u00edstica.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def gd(X, y, lambda_=0.01, tol=0.0000001, max_iter=10000):\n", "    theta = np.ones(X.shape[1])\n", "    old_err_sq = np.inf\n", "    i = 0\n", "    while True:\n", "        grad = derivadas(X, y, theta)\n", "        theta_novo = theta - lambda_ * grad\n", "        cross_entropy = cross_entropy_mean(X, y, theta_novo)\n", "        if np.abs(old_err_sq - cross_entropy) <= tol:\n", "            break\n", "        theta = theta_novo\n", "        old_err_sq = cross_entropy\n", "        i += 1\n", "        if i == max_iter:\n", "            break\n", "    return theta"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nTestando:\n<br>\n", "theta = gd(X_train, y_train)<br>\n", "r2_train = cross_entropy_mean(X_train, y_train, theta)<br>\n", "r2_test = cross_entropy_mean(X_test, y_test, theta)<br>\n", "assert_equal(np.round(r2_train,2), np.round(0.08605687702630486,2))<br>\n", "assert_equal(np.round(r2_test,2), np.round(0.18899648326443957,2))<br>\n", "# Gradiente Descendente Estoc\u00e1stico"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Relembrando a Aula 3:\n", "> Muitas vezes usaremos o gradiente descendente para escolher os par\u00e2metros de um modelo de maneira a minimizar alguma no\u00e7\u00e3o de erro. Usando a abordagem em lote anterior, cada etapa do m\u00e9todo exige que fa\u00e7amos uma previs\u00e3o e calculemos o gradiente para todo o conjunto de dados, o que faz com que cada etapa demore muito. <br>\n", "> <br> Geralmente, essas fun\u00e7\u00f5es de erro s\u00e3o *aditivas*, o que significa que o erro preditivo em todo o conjunto de dados \u00e9 simplesmente a soma dos erros preditivos para cada ponto de dados. <br>\n", "> <br> Quando este \u00e9 o caso, podemos, em vez disso, aplicar uma t\u00e9cnica chamada gradiente descendente estoc\u00e1stico (ou *stochastic gradient descent*), que calcula o gradiente (e d\u00e1 um passo) para apenas um ponto por vez. Ele passa sobre os dados repetidamente at\u00e9 atingir um ponto de parada."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exerc\u00edcio 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Modifique a implementa\u00e7\u00e3o de `sgd` para o caso da regress\u00e3o log\u00edstica.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sgd(X, y, lambda_=0.001, tol=0.01, max_iter=10000):\n", "  theta = np.ones(X.shape[1])\n", "  m = len(y)\n", "  for i in range(max_iter):\n", "      r = np.random.randint(m)\n", "      X_r, y_r = np.expand_dims(X[r], axis=0), np.array([y[r]])\n", "      grad = derivadas(X_r, y_r, theta)\n", "      theta = theta - lambda_ * grad\n", "      pred = sigmoid(np.dot(X_r, theta))\n", "      mean_err = np.mean((pred - y_r)**2)\n", "      if mean_err < tol:\n", "          break"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  return theta"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nTestando:\n<br>\n", "theta = sgd(X_train, y_train)<br>\n", "r2_train = cross_entropy_mean(X_train, y_train, theta)<br>\n", "r2_test = cross_entropy_mean(X_test, y_test, theta)<br>\n", "assert_equal(np.round(r2_train,2), np.round(0.09296116832663982,2))<br>\n", "assert_equal(np.round(r2_test,2), np.round(0.19610717861042887,2))<br>\n", "# sklearn: LogisticRegressionCV (somente didatico)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Por \u00faltimo, temos um exemplo de regress\u00e3o log\u00edstica com regulariza\u00e7\u00e3o L2 utilizando a biblioteca sklearn."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["O exemplo abaixo usa a vers\u00e3o com valida\u00e7\u00e3o cruzada, a fun\u00e7\u00e3o *LogisticRegressionCV*, cuja documenta\u00e7\u00e3o voc\u00ea encontra [aqui](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV). A maior diferen\u00e7a desta implementa\u00e7\u00e3o para as abordagens que estudamos na Aula 6 \u00e9 que a LogisticRegressionCV usa, por padr\u00e3o, um m\u00e9todo da fam\u00edlia quasi-Newton inv\u00e9s do gradiente descendente, o [Limited-memory BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Como grande parte das fun\u00e7\u00f5es implementadas no sklearn, a LogisticRegressionCV possui v\u00e1rios par\u00e2metros configur\u00e1veis, cuja melhor configura\u00e7\u00e3o vai depender do problema com o qual se est\u00e1 trabalhando. D\u00ea uma olhada na documenta\u00e7\u00e3o e tente brincar com as v\u00e1rias combina\u00e7\u00f5es de par\u00e2metros!\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegressionCV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf = LogisticRegressionCV(penalty='l2', cv=3).fit(X_train, y_train)\n", "predictions = clf.predict(X_test)\n", "print(clf.predict_proba(X_test).shape)\n", "# No caso deste modelo, o score padr\u00e3o computado \u00e9 a acur\u00e1cia (0, 1).\n", "# Podemos escolher outras fun\u00e7\u00f5es j\u00e1 implementadas ou\n", "# passar a nossa pr\u00f3pria m\u00e9trica como par\u00e2metro no momento da constru\u00e7\u00e3o\n", "# do regressor -> clf = LogisticRegressionCV(..., scoring=metrica_de_interesse)\n", "# m\u00e9tricas implementadas:\n", "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n", "print(clf.score(X_test, y_test))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf.coef_"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Cross-entropy em teste = \", cross_entropy_mean(X_test, y_test, clf.coef_[0]))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}