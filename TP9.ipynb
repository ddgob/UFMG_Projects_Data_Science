{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"Lista10.ipynb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Automatically generated by Colaboratory."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Original file is located at\n", "    https://colab.research.google.com/drive/1xr2oNSd1V8vndw9DzsDdVBH5_to6qSO7"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lista 10 - Regress\u00e3o M\u00faltipla"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Introdu\u00e7\u00e3o"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "# -*- coding: utf8<br>\n", "from scipy import stats as ss<br>\n", "import matplotlib.pyplot as plt<br>\n", "from numpy.testing import assert_equal, assert_array_almost_equal<br>\n", "import numpy as np<br>\n", "import pandas as pd<br>\n", "plt.ion()<br>\n", "import seaborn as sns<br>\n", "PARAMETROS_RC = {  # Par\u00e2metros RC para o matplotlib<br>\n", "  'figure.figsize': (12, 8),<br>\n", "  'axes.labelsize': 20,<br>\n", "  'axes.titlesize': 20,<br>\n", "  'legend.fontsize': 20,<br>\n", "  'xtick.labelsize': 20,<br>\n", "  'ytick.labelsize': 20,<br>\n", "  'lines.linewidth': 4,<br>\n", "}<br>\n", "sns.set(<br>\n", "    style=\"white\",<br>\n", "    palette=\"colorblind\",<br>\n", "    rc=PARAMETROS_RC,<br>\n", ")<br>\n", "def despine(ax=None):<br>\n", "    if ax is None:<br>\n", "        ax = plt.gca()<br>\n", "    # Hide the right and top spines<br>\n", "    ax.spines['right'].set_visible(False)<br>\n", "    ax.spines['top'].set_visible(False)<br>\n", "    # Only show ticks on the left and bottom spines<br>\n", "    ax.yaxis.set_ticks_position('left')<br>\n", "    ax.xaxis.set_ticks_position('bottom')<br>\n", "ontinuando da aula passada, vamos agora focar em casos mais espec\u00edficos de regress\u00e3o. Nesta aula, vamos estender a ideia de m\u00ednimos quadrados e de regress\u00e3o linear para modelos mais complexos. Para tal, vamos continuar nosso foco nos dados de pre\u00e7os de apartamentos em BH."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Dados"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Observe como temos 4 poss\u00edveis preditores de pre\u00e7o:\n", "1. \u00c1rea\n", "1. Quartos\n", "1. Su\u00edtes\n", "1. Vagas\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/aptosBH.txt', index_col=0)\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nCada preditor \u00e9 correlacionado com o pre\u00e7o de alguma forma. No pairplot abaixo, observe a \u00faltima coluna, onde cada linha corresponde a um preditor.\n<br>\n", "sns.pairplot(df, diag_kws={'edgecolor':'k'}, plot_kws={'alpha':0.5, 'edgecolor':'k'})<br>\n", "# Regress\u00e3o M\u00faltipla"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Para realizar uma regress\u00e3o m\u00faltipla, vamos representar nossos dados na forma matricial. Se $n$ \u00e9 o n\u00famero de linhas dos nossos dados (observa\u00e7\u00f5es) e $f$ \u00e9 o n\u00famero de colunas (features/atributos), ent\u00e3o os dados podem ser representados por:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$\n", "\\mathbf{X} = \\begin{pmatrix}\\mathbf {x} _{1}^{\\mathsf {T}}\\\\\\mathbf {x} _{2}^{\\mathsf {T}}\\\\\\vdots \\\\\\mathbf {x} _{n}^{\\mathsf {T}}\\end{pmatrix} =\\begin{pmatrix}1&x_{11}&\\cdots &x_{1f}\\\\1&x_{21}&\\cdots &x_{2f}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\1&x_{n1}&\\cdots &x_{nf}\\end{pmatrix}\n", "$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Observe como a primeira coluna \u00e9 toda $1$. A mesma vai capturar o fator constante, intercepto, da regress\u00e3o linear. O nosso modelo \u00e9 capturado pela equa\u00e7\u00e3o:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$y_i = \\theta_0 x_{i0} + \\theta_1 x_{i1}^{1} + \\theta_2 x_{i2}^{2} + \\cdots + \\theta_f x_{if}^{f} + \\epsilon_i$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Aqui, $x_{i0} = 1$ sempre (por isso usamos uma coluna de 1s). Dessa forma, a equa\u00e7\u00e3o acima pode ser simplificada para:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$y_i = \\theta_0 + \\theta_1 x_{i1}^{1} + \\theta_2 x_{i2}^{2} + \\cdots + \\theta_f x_{if}^{f} + \\epsilon_i$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Vamos organizar uma matriz de fatores explanat\u00f3rios X usando pandas.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = df['preco']\n", "X = df[['area', 'quartos', 'suites', 'vagas']]\n", "X['intercepto'] = 1\n", "X = X[['intercepto', 'area', 'quartos', 'suites', 'vagas']]\n", "X.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nTemos 216 observa\u00e7\u00f5es.\n<br>\n", "y.shape<br>\n", " atributos, onde um deles \u00e9 o intercepto.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nX.values retorna a matriz.\n<br>\n", "X = X.values<br>\n", "y = y.values # pegar a matrix<br>\n", "X<br>\n", "# Modelo"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Se $\\mathbf{y}$ \u00e9 a nossa resposta, ent\u00e3o o nosso modelo tem a seguinte forma matricial:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$\\mathbf {y} =\\mathbf{X}{\\boldsymbol {\\theta }}+{\\boldsymbol {\\varepsilon }} $$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Cada observa\u00e7\u00e3o $y_i$ vai ser capturada pelo modelo linear $y_i = \\theta_0 + \\theta_1 x_{i1}^{1} + \\theta_2 x_{i2}^{2} + \\cdots + \\theta_f x_{if}^{f} + \\epsilon_i$. Basta pensar em opera\u00e7\u00f5es de matrizes e vetores como somat\u00f3rios! Cada linha de $\\mathbf{X}$ multiplica o vetor de par\u00e2metros $\\theta$."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Soma dos erros quadrados"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["A fun\u00e7\u00e3o de perda na forma matricial \u00e9:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$L({\\boldsymbol {\\theta }}) = ||\\mathbf{y} - \\mathbf{X} {\\boldsymbol {\\theta }}||^2_2$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Essa fun\u00e7\u00e3o \u00e9 obtida atrav\u00e9s do quadrado da norma de Frobenius (com p=2), cuja f\u00f3rmula \u00e9 dada por:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$||A||_p = \\left(\\sum_i \\sum_j |a_{i,j}|^p\\right)^{\\frac{1}{p}}$$"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Jacobiana"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Quando temos v\u00e1rias derivadas de equa\u00e7\u00f5es na forma matricial, estamos computando a Jacobiana J (vetor de derivadas). Cada elemento do vetor J \u00e9 uma derivada:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$J(\\theta) =\n", "  \\left(\n", "    \\begin{matrix}\n", "    \\frac{dL}{d\\theta_0} & \\frac{dL}{d\\theta_1} & ... & \\frac{dL}{d\\theta_f}\n", "    \\end{matrix}\n", "  \\right)\n", "$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Um fator bastante interessante deste modelo \u00e9 que todas as derivadas (para cada $\\theta_i$) t\u00eam a mesma forma. Como temos uma soma de fatores lineares, cada $\\theta_i$ vai ter o mesmo formato. Assim:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$\\frac{dL}{d\\theta_j} = -2n^{-1} \\sum_{i=1}^{n} \\left(y_i - \\sum_{j=0} \\theta_j x_{ij}\\right) x_{ij}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$\\frac{dL}{d\\theta_j} = -2n^{-1} \\sum_{i=1}^{n} \\epsilon_i x_{ij}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["A fun\u00e7\u00e3o abaixo computa tal derivada explorando o conceito de vetoriza\u00e7\u00e3o.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def derivadas_regressao_media_old(theta, X, y):\n", "    return -2 * ((y - X @ theta) * X.T).mean(axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def derivadas_regressao(theta, X, y):\n", "    return -2 * ((y - X @ theta) @ X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Vers\u00e3o nova da fun\u00e7\u00e3o da m\u00e9dia\n<br>\n", "def derivadas_regressao_media(theta, X, y):<br>\n", "    return -2 * ((y - X @ theta) @ X) / len(y)<br>\n", "# Gradiente Descendente"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Ap\u00f3s definir o modelo, a fun\u00e7\u00e3o de erro e a Jacobiana para a Regress\u00e3o M\u00faltipla, podemos calcular o gradiente descendente!\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def gd(theta, d_fun, X, y, lambda_=0.0001, tol=0.00001, max_iter=10000):\n", "  theta = theta.copy()\n", "  #print('Iter {}; theta = '.format(0), theta)\n", "  old_err_sq = np.inf\n", "  i = 0\n", "  while True:\n", "    # Computar as derivadas\n", "    grad = d_fun(theta, X, y)\n", "    # Atualizar\n", "    theta_novo = theta - lambda_ * grad\n\n", "    # Parar quando o erro convergir\n", "    err_sq = ((X.dot(theta) - y) ** 2).mean()\n", "    if np.abs(old_err_sq - err_sq) <= tol:\n", "      break\n", "    theta = theta_novo\n", "    old_err_sq = err_sq\n", "    #print('Iter {}; theta = '.format(i+1), theta)\n", "    i += 1\n", "    if i == max_iter:\n", "      break\n", "  return theta"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Gradiente Descendente Estoc\u00e1stico<br>\n", "O gradiente descendente estoc\u00e1stico tamb\u00e9m funciona! Entretanto, temos que acertar a taxa de aprendizado.<br>\n", "Um dos problemas do uso recorrente do gradiente descendente estoc\u00e1stico \u00e9 que n\u00e3o sabemos ainda como acertar nossa taxa. Para tal, geralmente fazemos uso de treino/valida\u00e7\u00e3o/teste (assunto das pr\u00f3ximas aulas).<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sgd(theta, d_fun, X, y, lambda_=0.001, tol=0.01, max_iter=10000):\n", "  theta = theta.copy()\n", "  #print('Iter {}; alpha, beta = '.format(0), theta)\n", "  old_err_sq = np.inf\n", "  for i in range(max_iter):\n", "    # Escolhe ponto aleat\u00f3rio\n", "    r = np.random.randint(len(y))\n", "    X_r, y_r = X[r], y[r]\n", "    X_r = X_r.reshape(1, len(X_r)) # transforma o vetor linha em matriz\n\n", "    # Deriva e atualiza\n", "    grad = d_fun(theta, X_r, y_r)\n", "    theta_novo = theta - lambda_ * grad\n\n", "    #Calcula o erro\n", "    err_sq = ((X.dot(theta) - y) ** 2).mean()\n", "    theta = theta_novo\n", "    if err_sq < tol:\n", "      break\n", "      #print('Iter {}; alpha, beta = '.format(i+1), theta)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  return theta"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Normaliza\u00e7\u00e3o dos Dados<br>\n", "A normaliza\u00e7\u00e3o dos dados pode ser \u00fatil para ajudar o gradiente. O algoritmo funciona sem tal passo, por\u00e9m \u00e9 mais chato definir uma taxa de aprendizado sem normaliza\u00e7\u00e3o e a converg\u00eancia \u00e9 mais lenta.<br>\n", "Quando as features aparecem com ordens de grandeza muito diferentes (ex: idade entre 0 a 100 e renda mensal entre  800 e 100.000 reais) a fun\u00e7\u00e3o de custo \u00e9 distorcida, tornando o ponto m\u00ednimo dif\u00edcil de alcan\u00e7ar. Sendo assim, um truque importante \u00e9 garantir que todos as features estejam em uma escala similar.<br>\n", "Abaixo normalizamos as features dos dados de pre\u00e7os de apartamentos em BH e, em seguida, calculamos o gradiente descendente e o gradiente descendente estoc\u00e1stico.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/aptosBH.txt', index_col=0)\n", "z_df = (df-df.mean())/df.std()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = z_df['preco']\n", "X = z_df[['area', 'quartos', 'suites', 'vagas']]\n", "X['intercepto'] = 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = X.values\n", "y = y.values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["theta = np.ones(5)\n", "theta = gd(theta, derivadas_regressao, X, y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"theta = \", theta)\n", "print()\n", "print(X[0:4])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/aptosBH.txt', index_col=0)\n", "z_df = (df-df.mean())/df.std()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = z_df['preco']\n", "X = z_df[['area', 'quartos', 'suites', 'vagas']]\n", "X['intercepto'] = 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = X.values\n", "y = y.values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["theta = np.ones(5)\n", "theta = sgd(theta, derivadas_regressao, X, y, lambda_=0.002)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"theta = \", theta)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nObserve como os resultados batem com o scikit learn.\n<br>\n", "from sklearn.linear_model import LinearRegression<br>\n", "model = LinearRegression(fit_intercept=True)<br>\n", "model.fit(X, y)<br>\n", "model.coef_<br>\n", "model.intercept_<br>\n", "# Erros e Valida\u00e7\u00e3o"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Ainda podemos usar o R-quadrado para avaliar a qualidade de uma regress\u00e3o linear m\u00faltipla. A interpreta\u00e7\u00e3o permanece a mesma: quanto da vari\u00e2ncia dos dados \u00e9 capturada pelo modelo.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def erro(y, X, theta):\n", "  return y - X@theta"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def multiple_r_squared(X, y, theta):\n", "  sse = sum(erro(y, X, theta)**2)\n", "  sst = sum((y - np.mean(y))**2)\n", "  return 1.0 - sse / sst"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/aptosBH.txt', index_col=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = df['preco']\n", "X = df[['area', 'quartos', 'suites', 'vagas']]\n", "X['intercepto'] = 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = X.values\n", "y = y.values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = LinearRegression(fit_intercept=False)\n", "model.fit(X, y)\n", "print(model.coef_)\n", "print(\"R2 sem normalizacao = \", model.score(X, y))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/aptosBH.txt', index_col=0)\n", "z_df=(df-df.mean())/df.std()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = z_df['preco']\n", "X = z_df[['area', 'quartos', 'suites', 'vagas']]\n", "X['intercepto'] = 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = X.values\n", "y = y.values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = LinearRegression(fit_intercept=False)\n", "model.fit(X, y)\n", "print(model.coef_)\n", "print(\"R2 com normalizacao = \", model.score(X, y))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Bootstrap para entender a import\u00e2ncia de fatores<br>\n", "Podemos aplicar um bootstrap para entender a import\u00e2ncia dos fatores. N\u00f3s repetidamente tomamos um `bootstrap_regression` dos nossos dados e estimamos o vetor $\\Theta$ com base nessa amostra. Se o coeficiente correspondente a uma das vari\u00e1veis independentes n\u00e3o variar muito entre as amostras, podemos ter certeza de que nossa estimativa \u00e9 relativamente segura. Se o coeficiente variar muito entre as amostras, n\u00e3o podemos ficar confiantes em nossa estimativa. Caso o intervalo capture o zero, o fator n\u00e3o importa do ponto de vista estat\u00edstico. Vamos fazer uso de scikit learn.<br>\n", "Vamos ver exemplos nos dados dos apartamentos.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def bootstrap_regression(X, y, n=10000, size=None):\n", "  if size is None:\n", "    size = len(df)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  T = np.zeros(shape=(n, X.shape[1]))\n", "  for i in range(n):\n", "    # Gera amostras com reposi\u00e7\u00e3o\n", "    idx = np.random.choice(len(y), len(y))\n", "    Xb = X[idx]\n", "    yb = y[idx]\n\n", "    # Fit usando sklearn\n", "    model = LinearRegression(fit_intercept=True)\n", "    model.fit(Xb, yb)\n", "    T[i] = model.coef_\n", "  return T"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nA seguir, n\u00e3o usamos o scikit learn e eliminamos duplicatas da amostra:\n<br>\n", "def bootstrap_regression_gd(X, y, n=10000, size=None):<br>\n", "  if size is None:<br>\n", "    size = len(df)<br>\n", "  T = np.zeros(shape=(n, X.shape[1]))<br>\n", "  for i in range(n):<br>\n", "    if i%400 == 0:<br>\n", "      print(\"bootstrap iter \", i)<br>\n", "    # Gera amostras com reposi\u00e7\u00e3o e remove duplicatas<br>\n", "    idx = list(np.random.choice(len(y), len(y)))<br>\n", "    Xb = X[idx]<br>\n", "    yb = y[idx]<br>\n", "    # Fit usando gd<br>\n", "    theta = np.ones(X.shape[1])<br>\n", "    theta = gd(theta, derivadas_regressao, Xb, yb)<br>\n", "    T[i] = theta<br>\n", "  return T<br>\n", "y = z_df['preco'].values<br>\n", "names = ['area', 'quartos', 'suites', 'vagas']<br>\n", "X = z_df[names]<br>\n", "X['intercepto'] = 1<br>\n", "X = X.values<br>\n", "T = bootstrap_regression_gd(X, y)<br>\n", "names = ['area', 'quartos', 'suites', 'vagas', 'intercepto']<br>\n", "def plota_bootstrap_para_iesimo_parametro(T, i):<br>\n", "  fig = plt.figure()<br>\n", "  plt.hist(T[:, i], edgecolor='k')<br>\n", "  plt.title('{} - 95% CI Bootstrap: ({:.2f}, {:.2f})'.format(names[i],<br>\n", "                                                              np.percentile(T[:, i], 2.5),<br>\n", "                                                              np.percentile(T[:, i], 97.5)))<br>\n", "  plt.xlabel(r'$\\theta_i$')<br>\n", "  plt.ylabel('# Boot Amostras')<br>\n", "  despine()<br>\n", "  plt.show()<br>\n", "plota_bootstrap_para_iesimo_parametro(T, 0)<br>\n", "#  Exerc\u00edcios - Outros datasets:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Carros"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Utilizando a base de dados carros, ``hybrid.csv``, vamos fazer um gradiente descendente para uma regress\u00e3o linear com m\u00faltiplas vari\u00e1veis. As colunas s\u00e3o definidas da seguinte forma:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["- ve\u00edculo (vehicle): modelo do carro"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["- ano (year): ano de fabrica\u00e7\u00e3o"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["- msrp: pre\u00e7o de varejo em d\u00f3lar sugerido pelo fabricante em 2013."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["- acelera\u00e7\u00e3o (acceleration): taxa de acelera\u00e7\u00e3o em km por hora por segundo"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["- mpg: economia de combust\u00edvel em milhas por gal\u00e3o"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["- classe (class): a classe do modelo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Nosso objetivo ser\u00e1 estimar o valor de pre\u00e7o sugerido dos carros a partir dos demais atributos (exluindo o nome do ve\u00edculo e a classe).\n", "Portanto, teremos a regress\u00e3o definida pela f\u00f3rmula:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$ Y = X\\Theta + \\epsilon $$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["onde, Y corresponde \u00e0 coluna ``msrp`` dos dados, e X corresponde \u00e0s colunas ``year,acceleration,mpg``."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Observe a forma dos dados e a correla\u00e7\u00e3o entre as vari\u00e1veis:\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/hybrid.csv')\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import seaborn as sns\n", "sns.pairplot(df, diag_kws={'edgecolor':'k'}, plot_kws={'alpha':0.5, 'edgecolor':'k'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n**Exerc\u00edcio 1:** Selecione apenas as colunas que ser\u00e3o utilizadas e normalize os dados para que o gradiente descendente rode sem problemas.\n<br>\n", "# SELECIONE AS COLUNAS<br>\n", "X = df[[\"year\", \"acceleration\", \"mpg\"]]<br>\n", "y = df[\"msrp\"]<br>\n", "# NORMALIZE OS DADOS<br>\n", "X = (X - X.mean()) / X.std(ddof=1)<br>\n", "y = (y - y.mean()) / y.std(ddof=1)<br>\n", "# ACRESCENTE INTERCEPTO<br>\n", "inter = np.ones(y.size)<br>\n", "X.insert(0, \"Intercepto\", inter, True)<br>\n", "# EXTRAIA MATRIZES COM .VALUES<br>\n", "X = X.values<br>\n", "y = y.values<br>\n", "*Exerc\u00edcio 2:** Implemente a fun\u00e7\u00e3o de gradiente dos par\u00e2metros da regress\u00e3o, retornando um array com os valores dos gradientes para cada par\u00e2metro.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def gradients(theta, X, y):\n", "  # YOUR CODE HERE\n", "  # x : matriz nxm\n", "  # y : array nx1\n", "  # theta : array mx1\n", "  return -2 * ((y - X @ theta) @ X) / len(y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n* Implemente a fun\u00e7\u00e3o de gradiente descendente para os par\u00e2metros da regress\u00e3o, retornando um array com os valores de alpha e os valores de beta para cada coluna. Voc\u00ea deve usar a fun\u00e7\u00e3o `gradients` definida anteriormente.\n<br>\n", "def descent(theta0, X, y, learning_rate=0.005, tolerance=0.0000001):<br>\n", "    oldErr = np.inf<br>\n", "    i = 0<br>\n", "    for count in range(10000):<br>\n", "        grads = gradients(theta0, X, y)<br>\n", "        newTheta = theta0 - learning_rate * grads<br>\n", "        newErr = ((X.dot(newTheta) - y) ** 2).mean()<br>\n", "        if np.abs(oldErr - newErr) <= tolerance:<br>\n", "            break<br>\n", "        theta0 = newTheta<br>\n", "        oldErr = newErr<br>\n", "    return newTheta<br>\n", "_teste_param0 = np.array([ 1000, 1, 1, 1 ])<br>\n", "theta = descent(_teste_param0, X, y)<br>\n", "assert_array_almost_equal(np.round(theta,4), np.round([ 0.00201339, -0.04349112,  0.59055261, -0.23979036],4))<br>\n", "*Exerc\u00edcio 3:** Agora vamos tentar avaliar o modelo de regress\u00e3o linear obtido com o gradiente descendente. Para isso, implemente a fun\u00e7\u00e3o que calcula o R-quadrado."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Lembre-se que, para calcular o R-quadrado, voc\u00ea precisa do valor da soma total dos quadrados e da soma dos erros quadrados. Os par\u00e2metros de entrada de cada fun\u00e7\u00e3o s\u00e3o dados.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sst(y):\n", "  # YOUR CODE HERE\n", "  return ((y - y.mean()) ** 2).sum()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict(X, theta):\n", "  # YOUR CODE HERE\n", "  return X @ theta.T"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sse(X, y, theta):\n", "  # YOUR CODE HERE\n", "  return y.T @ y - (y.T @ X @ np.linalg.inv(X.T @ X) @ X.T @ y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def rsquared(X, y, theta):\n", "  # YOUR CODE HERE\n", "  return 1 - (sse(X, y, theta) / sst(y))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rs = rsquared(X, y, theta)\n", "assert_equal(np.round(rs,4), np.round(0.5288887684860548,4))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nSe observarmos os dados pelos gr\u00e1ficos que mostram as correla\u00e7\u00f5es entre as vari\u00e1veis, podemos perceber que nem todos possuem uma rela\u00e7\u00e3o linear. Vamos tentar transformar os dados de um dos atributos dos carros, para que uma regress\u00e3o linear possa ser aplicada com melhores resultados.<br>\n", "**Exerc\u00edcio 4:** Para isso, tire o logaritmo da vari\u00e1vel ```mpg``` e, em seguida, z-normalize os dados (todos as vari\u00e1veis em X).<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = df[\"msrp\"]\n", "X = df[[\"year\", \"acceleration\", \"mpg\"]]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["TIRE O LOG DA VARIAVEL MPG"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["aux = X.copy()\n", "for i in range(len(y)):\n", "    aux.iloc[i, 2] = np.log(aux.iloc[i, 2])\n", "X = aux"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Z-NORMALIZE OS DADOS"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dfNormalized = pd.DataFrame()\n", "numericColumns = X.select_dtypes(include=np.number).columns\n", "means = X[numericColumns].mean()\n", "stds = X[numericColumns].std()\n", "dfNormalized[numericColumns] = (X[numericColumns] - means) / stds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = dfNormalized"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ADICIONE O INTERCEPTO"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["inter: np.ndarray = np.ones(y.size)\n", "X.insert(0, \"Intercepto\", inter, True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["DEFINA X e Y UTILIZANDO .VALUES"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = X.values\n", "y = y.values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n* Rode novamente o c\u00f3digo do gradiente descendente e verifique se o R-quadrado da regress\u00e3o melhorou ou piorou ap\u00f3s a transforma\u00e7\u00e3o dos dados.\n<br>\n", "_teste_param0 = np.array([ 1000, 1, 1, 1 ])<br>\n", "theta = descent(_teste_param0, X, y)<br>\n", "rs1 = rsquared(X, y, theta)<br>\n", "assert_equal(np.round(rs1,4), np.round(0.5543728866213389,4))<br>\n", "## ATEN\u00c7\u00c3O"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Os pr\u00f3ximos dois exerc\u00edcios n\u00e3o possuem corre\u00e7\u00e3o autom\u00e1tica, por\u00e9m s\u00e3o de vital import\u00e2ncia. Ambos os datasets utilizados nos exerc\u00edcios a seguir s\u00e3o datasets com grande n\u00famero de vari\u00e1veis, o que se aproxima de problemas reais. Como a realiza\u00e7\u00e3o de testes com os valores \u00e9 dificultada devido ao alto n\u00famero de vari\u00e1veis, optou-se pela n\u00e3o corre\u00e7\u00e3o autom\u00e1tica."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Tente encontrar o modelo que melhor se ajusta aos dados, com maior valor de $R^2$. Tente tamb\u00e9m compreender a interpreta\u00e7\u00e3o dos coeficientes para cada vari\u00e1vel explicativa. Em caso de d\u00favidas, acione o monitor da disciplina."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Dataset Rea\u00e7\u00f5es Qu\u00edmicas - SEM CORRE\u00c7\u00c3O AUTOM\u00c1TICA"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Um cientista maluco misturou quantidades (em mil\u00edmetros c\u00fabicos) aleat\u00f3rias de diversos reagentes e conseguiu gerar uma determinada quantidade (em mil\u00edmetros c\u00fabicos) de um produto qu\u00edmico, que embora n\u00e3o sirva para nada, n\u00e3o \u00e9 t\u00f3xico e \u00e9 muito bonito. O problema \u00e9 que o cientista n\u00e3o sabe quais reagentes est\u00e3o, de fato, reagindo e gerando o tal produto. Mais importante, ele n\u00e3o sabe a quantidade que ele tem que colocar de cada reagente para gerar uma dada quantidade do produto. Para descobrir, ele misturou quantidades aleat\u00f3rias de cada reagente 1000 vezes e anotou a quantidade do produto que foi gerada em cada um desses experimentos. Para encontrar a f\u00f3rmula m\u00e1gica t\u00e3o desejada, ele pediu a sua ajuda. Os experimentos est\u00e3o descritos nas linhas do arquivo. A quantidade gerada do produto para cada experimento est\u00e1 descrita na linha \"q_produto\"."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["E a\u00ed? Ser\u00e1 que regress\u00e3o linear m\u00faltipla resolve? O professor sabe que resolve, e tem um modelo com $R^2$ superior a $0.99$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["* Encontre esse modelo e mostre os coeficientes, a qualidade do ajuste, e os fatores significativos. Voc\u00ea pode utilizar as fun\u00e7\u00f5es definidas anteriormente, ou utilizar a biblioteca sklearn.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/chemical_reaction.csv')\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "X = df.iloc[:, 1:].values\n", "y = df.iloc[:, 0].values\n", "modelo = LinearRegression()\n", "modelo.fit(X, y)\n", "print(\"Coeficientes: {}\".format(modelo.coef_))\n", "print(\"Intercept: {}\".format(modelo.intercept_))\n", "print(\"R^2: {}\".format(modelo.score(X, y)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n### Dataset FIFA - SEM CORRE\u00c7\u00c3O AUTOM\u00c1TICA<br>\n", "O [FIFA](https://pt.wikipedia.org/wiki/FIFA_(s%C3%A9rie)) \u00e9 um jogo eletr\u00f4nico de futebol muito popular em que, parte da divers\u00e3o, \u00e9 poder jogar partidas envolvendo jogadores reais. Para simular um jogador real, o jogo descreve-os usando diversos atributos (como poder do chute, velocidade, etc) os quais, de alguma forma, representam em sua habilidade como um todo (*overall*).<br>\n", "* Neste exerc\u00edcio voc\u00ea deve encontrar um modelo linear que explique a habilidade *overall* dos jogadores do FIFA a partir das suas outras habilidades. Para isso, utilize o arquivo contendo as habilidades de 20 mil jogadores. Mostre os coeficientes, a qualidade do ajuste e os fatores significativos.<br>\n", "**Dica:** pode haver mais de um modelo!<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/skills_and_overall_sample_20k.csv', index_col=0)\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["numericColumns = df.select_dtypes(include=np.number).columns\n", "dfNumeric = df[numericColumns]\n", "X = dfNumeric.iloc[:, 1:].values\n", "y = dfNumeric.iloc[:, 0].values\n", "modelo = LinearRegression()\n", "modelo.fit(X, y)\n", "print(\"Coeficientes: {}\".format(modelo.coef_))\n", "print(\"Intercept: {}\".format(modelo.intercept_))\n", "print(\"R^2: {}\".format(modelo.score(X, y)))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}