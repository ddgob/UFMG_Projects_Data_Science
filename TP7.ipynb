{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"Lista07.ipynb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Automatically generated by Colaboratory."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Original file is located at\n", "    https://colab.research.google.com/drive/1-86KFeuAsiZ78ESCjVYRoSTfJKBwMMYJ"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Imports"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "# -*- coding: utf8<br>\n", "from scipy import stats as ss<br>\n", "from numpy.testing import assert_equal<br>\n", "import matplotlib.pyplot as plt<br>\n", "import numpy as np<br>\n", "import pandas as pd<br>\n", "plt.ion()<br>\n", "import seaborn as sns<br>\n", "PARAMETROS_RC = {  # Par\u00e2metros RC para o matplotlib<br>\n", "  'figure.figsize': (12, 8),<br>\n", "  'axes.labelsize': 20,<br>\n", "  'axes.titlesize': 20,<br>\n", "  'legend.fontsize': 20,<br>\n", "  'xtick.labelsize': 20,<br>\n", "  'ytick.labelsize': 20,<br>\n", "  'lines.linewidth': 4,<br>\n", "}<br>\n", "sns.set(<br>\n", "    style=\"white\",<br>\n", "    palette=\"colorblind\",<br>\n", "    rc=PARAMETROS_RC,<br>\n", ")<br>\n", "def despine(ax=None):<br>\n", "    if ax is None:<br>\n", "        ax = plt.gca()<br>\n", "    # Hide the right and top spines<br>\n", "    ax.spines['right'].set_visible(False)<br>\n", "    ax.spines['top'].set_visible(False)<br>\n", "    # Only show ticks on the left and bottom spines<br>\n", "    ax.yaxis.set_ticks_position('left')<br>\n", "    ax.xaxis.set_ticks_position('bottom')<br>\n", " Lista 07 - Regress\u00e3o Linear Simples"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Introdu\u00e7\u00e3o"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Antes de iniciar o nosso estudo da regress\u00e3o, vamos pensar em um modelo que tenta capturar o valor m\u00e9dio no eixo y. Para uma base de dados unidimensional (apenas Y), a m\u00e9dia ($\\bar{y}$) \u00e9 um bom estimador. Por exemplo, imagine que voc\u00ea est\u00e1 em uma sala de aula e entra um novo aluno na sala. \u00c9 poss\u00edvel prever alguma caracter\u00edstica deste aluno (como o rendimento escolar) usando a m\u00e9dia dos outros alunos."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Agora, como podemos proceder quando temos duas vari\u00e1veis X e Y? Podemos usar a fun\u00e7\u00e3o de correla\u00e7\u00e3o para medir a for\u00e7a da rela\u00e7\u00e3o linear entre duas vari\u00e1veis. Para a maioria das aplica\u00e7\u00f5es, saber que esse relacionamento linear existe n\u00e3o \u00e9 suficiente. Queremos ser capazes de entender a natureza do relacionamento. \u00c9 aqui que vamos usar a regress\u00e3o linear simples."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Vamos olhar para uma das bases dados que ajudaram a criar a regress\u00e3o linear."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Poor Person's KNN  (Vizinhos Pr\u00f3ximos Implementado de Forma Simples)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Vamos iniciar com a base de dados original analisada por [Galton](https://en.wikipedia.org/wiki/Francis_Galton). Tal base captura a altura de um filho, ap\u00f3s alguns anos, com base na altura m\u00e9dia dos pais. O problema era prever qual vai ser a altura futura de uma crian\u00e7a usando apenas a altura dos pais como entrada. Como temos um pai e uma m\u00e3e, usamos a m\u00e9dia das alturas entre os dois."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Especificamente, temos que:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["1. X \u00e9 a altura m\u00e9dia dos pais\n", "1. Y \u00e9 a altura da crian\u00e7a ap\u00f3s alguns anos (o mesmo n\u00famero de anos para toda crian\u00e7a)\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/galton.csv', index_col=0)\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nAbaixo temos a dispers\u00e3o dos dados\n<br>\n", "x = df['midparentHeight'].values<br>\n", "y = df['childHeight'].values<br>\n", "plt.scatter(x, y, alpha=0.8, edgecolors='k', s=80)<br>\n", "plt.xlabel('Altura m\u00e9dia dos pais')<br>\n", "plt.ylabel('Altura dos filhos')<br>\n", "plt.title('Regress\u00e3o linear simples')<br>\n", "despine()<br>\n", "ma forma de capturar a correla\u00e7\u00e3o \u00e9 fazer uma regress\u00e3o de vizinhos pr\u00f3ximos (nearest neighbors). Para tal, precisamos agrupar, no eixo-x, pontos pr\u00f3ximos um dos outros. \u00c9 poss\u00edvel fazer o mesmo nos eixos x e y usando uma dist\u00e2ncia euclideana. Por simplicidade vamos agrupar apenas em x. No caso particular destes dados onde os valores s\u00e3o bem espalhados no eixo x, podemos simplesmente arredondar os n\u00fameros\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["copy = df[['midparentHeight', 'childHeight']].copy()\n", "copy['midparentHeight'] = copy['midparentHeight'].round()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nAgrupar por x e tirar a m\u00e9dia de y.\n<br>\n", "model = copy.groupby('midparentHeight').mean()<br>\n", "model.head()<br>\n", "bservando o modelo:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_nn = model.index\n", "y_nn = model.values\n", "plt.scatter(x, y, alpha=0.8, edgecolors='k', s=80)\n", "plt.plot(x_nn, y_nn, color='magenta')\n", "plt.xlabel('Altura m\u00e9dia dos pais')\n", "plt.ylabel('Altura dos filhos')\n", "plt.title('Regress\u00e3o linear simples')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nPrevis\u00e3o: Para um novo ponto, arredondar seu peso. Ver valor de y_nn\n<br>\n", "altura_media_pais = 71<br>\n", "model.loc[71.0]<br>\n", "bserve que o modelo acima \u00e9 muito perto de uma reta. Ele ilustra as principais ideias necess\u00e1rias para entender a correla\u00e7\u00e3o linear:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["1. Capturar a m\u00e9dia de valores pr\u00f3ximos um dos outros em x\n", "1. Capturar a m\u00e9dia de valores pr\u00f3ximos um dos outros em y"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Resolvemos o (1) problema nos dados acima com um round. No caso (2) com um groupby e mean."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Observe como o modelo \u00e9 quase o mesmo quando z-normalizamos os dados.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["znormed = df[['midparentHeight', 'childHeight']].copy()\n", "znormed = (znormed - znormed.mean()) / znormed.std(ddof=1)\n", "znormed.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = znormed['midparentHeight'].copy()\n", "y = znormed['childHeight'].copy()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["znormed['midparentHeight'] = znormed['midparentHeight'].round()\n", "model = znormed.groupby('midparentHeight').mean()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_nn = model.index\n", "y_nn = model.values\n", "plt.scatter(x, y, alpha=0.8, edgecolors='k', s=80)\n", "plt.plot(x_nn, y_nn, color='magenta')\n", "plt.xlabel('Altura m\u00e9dia dos pais')\n", "plt.ylabel('Altura dos filhos')\n", "plt.title('Regress\u00e3o linear simples')\n", "despine()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Regress\u00e3o Linear Simples<br>\n", "Nos slides da aula, leia a discuss\u00e3o sobre como uma regress\u00e3o linear tenta capturar a mesma ideia do nosso KNN simples acima. De forma simples, queremos uma reta capaz de explicar tando a dispers\u00e3o em x quanto em y. Tal reta deve se aproximar da m\u00e9dia dos dois valores quando vistos em janelas (estilo o NN acima). Em particular, voc\u00ea sup\u00f5e que existem constantes $\\alpha$ (alfa) e $\\beta$ (beta) tais que:<br>\n", "$$\\hat{y}_i = \\alpha + \\beta x_i + \\epsilon_i$$<br>\n", "onde $\\epsilon_i$ \u00e9 um termo de erro (esperan\u00e7osamente pequeno) que representa o fato de que existem outros fatores n\u00e3o explicados por este modelo simples. Idealmente, essa estimativa consegue capturar o valor m\u00e9dio de $y_i$ para grupos de valores pr\u00f3ximos em $x_i$.<br>\n", "## Exerc\u00edcio 1: Reta<br>\n", "* Supondo que tenhamos determinado $\\alpha$, $\\beta$ e $x_i$, implemente uma fun\u00e7\u00e3o que recebe como par\u00e2metro os valores de $\\alpha$, $\\beta$ e $x_i$ e retorna o valor predito, $\\hat{y}_i$.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alpha = 1.5\n", "beta = 0.8\n", "x = 10"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict(alpha, beta, x_i):\n", "  predicao = alpha + beta * x_i\n", "  return predicao"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["foo = predict(alpha, beta, x)\n", "assert_equal(foo, 9.5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Exerc\u00edcio 2: Erro<br>\n", "**Como escolhemos alfa e beta?**<br>\n", "Bem, qualquer escolha de alfa e beta nos d\u00e1 uma sa\u00edda prevista para cada entrada $x_i$. Como sabemos a sa\u00edda real $y_i$, podemos calcular o erro de cada par.<br>\n", "* Implemente uma fun\u00e7\u00e3o que recebe como par\u00e2metro os valores de $\\alpha$, $\\beta$, $x$ e $y$ e retorna o erro da sa\u00edda prevista pelo modelo.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = 10.2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def error(alpha, beta, x, y):\n", "  erro = y - predict(alpha, beta, x)\n", "  return erro"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["erro = error(alpha, beta, x, y)\n", "assert_equal(round(erro, 2), 0.7)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Exerc\u00edcio 3: Erro m\u00e9dio<br>\n", "* Voltando ao exemplo que utiliza a base de dados original analisada por Galton, utilize a fun\u00e7\u00e3o de erro implementada e calcule o erro m\u00e9dio do modelo. Considere $x$ a altura m\u00e9dia dos pais e $y$ a altura da crian\u00e7a.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = df['midparentHeight']\n", "y = df['childHeight']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n* Inicialmente, considere $\\alpha$ = 1 e $\\beta$ = 0.5.\n<br>\n", "alpha = 1<br>\n", "beta = 0.5<br>\n", "def mean_error(alpha, beta, x, y):<br>\n", "  somaErros = 0<br>\n", "  for x_i, y_i in zip(x, y):<br>\n", "    somaErros += error(alpha, beta, x_i, y_i)<br>\n", "  return somaErros/len(x)<br>\n", "erro_medio1 = mean_error(alpha, beta, x, y)<br>\n", "assert_equal(round(erro_medio1, 2), 31.14)<br>\n", " A seguir, mostre o gr\u00e1fico de dispers\u00e3o dos dados e a reta gerada pelo modelo. (SEM CORRE\u00c7\u00c3O AUTOM\u00c1TICA)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.scatter(x, y, alpha=0.8, edgecolors='k', s=80)\n", "x_model = np.linspace(64, 76, 100)\n", "y_model = alpha + x_model * beta\n", "plt.plot(x_model, y_model, color='magenta')\n", "plt.xlabel('Altura m\u00e9dia dos pais')\n", "plt.ylabel('Altura dos filhos')\n", "plt.title('Regress\u00e3o linear simples')\n", "despine()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n* Repita o c\u00e1lculo considerando $\\alpha$ = 22 e $\\beta$ = 0.5.\n<br>\n", "alpha = 22<br>\n", "beta = 0.5<br>\n", "erro_medio2 = mean_error(alpha, beta, x, y)<br>\n", "assert_equal(round(erro_medio2, 2), 10.14)<br>\n", " A seguir, mostre o gr\u00e1fico de dispers\u00e3o dos dados e a reta gerada pelo modelo. (SEM CORRE\u00c7\u00c3O AUTOM\u00c1TICA)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.scatter(x, y, alpha=0.8, edgecolors='k', s=80)\n", "x_model = np.linspace(64, 76, 100)\n", "y_model = alpha + x_model * beta\n", "plt.plot(x_model, y_model, color='magenta')\n", "plt.xlabel('Altura m\u00e9dia dos pais')\n", "plt.ylabel('Altura dos filhos')\n", "plt.title('Regress\u00e3o linear simples')\n", "despine()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nVoc\u00ea deve notar que o segundo modelo \u00e9 bem melhor. Por\u00e9m os dois s\u00e3o p\u00e9ssimos.<br>\n", "## Exerc\u00edcio 4: Soma dos erros quadrados<br>\n", "O que realmente gostar\u00edamos de saber \u00e9 o erro total em todo o conjunto de dados. Inicialmente, pode-se pensar que a soma dos erros nos daria esse valor, por\u00e9m esse n\u00e3o \u00e9 o caso. Se a previs\u00e3o para um dado $x_1$ for muito alta (erro positivo) e a previs\u00e3o para $x_2$ for muito baixa (erro negativo), esses erros se cancelariam na soma.<br>\n", "A corre\u00e7\u00e3o mais simples para esse problema seria considerar a soma dos erros absolutos. Contudo, esse tipo de erro ($\\vert x \\vert$) n\u00e3o \u00e9 interessante de um ponto de vista matem\u00e1tico, uma vez que n\u00e3o \u00e9 bem comportado (n\u00e3o possui derivada) em $x=0$. No geral, aprendizado de m\u00e1quina depende da minimiza\u00e7\u00e3o (ou maximiza\u00e7\u00e3o) de fun\u00e7\u00f5es. Como deve ter aprendido nos cursos de c\u00e1lculo, esses pontos extremos est\u00e3o diretamente relacionados \u00e0 no\u00e7\u00e3o de derivada (ou gradiente) e, assim, a n\u00e3o exist\u00eancia da derivada em $x=0$ \u00e9 um grave problema.<br>\n", "De qualquer forma, ainda existem v\u00e1rias formas de calcular erros sem as dificuldades discutidas acima: qualquer erro n\u00e3o-negativo e bem-comportado para todos os n\u00fameros reais seria uma solu\u00e7\u00e3o v\u00e1lida. Dentre eles, o mais comum \u00e9 o erro quadrado. Essa n\u00e3o \u00e9 uma escolha puramente arbitr\u00e1ria, j\u00e1 que no caso da regress\u00e3o linear existe uma conex\u00e3o entre minimiza\u00e7\u00e3o da soma dos erros quadrados e o processo (ainda a ser estudado) de m\u00e1xima verossimilhan\u00e7a.<br>\n", "- Implemente uma fun\u00e7\u00e3o que recebe como par\u00e2metro os valores de $\\alpha$, $\\beta$, $x$ e $y$ e retorna a soma dos erros quadrados da sa\u00edda prevista pelo modelo.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sum_of_squared_errors(alpha, beta, x, y):\n", "  somaErrQuad = 0\n", "  for x_i, y_i in zip(x, y):\n", "    somaErrQuad += error(alpha, beta, x_i, y_i) ** 2\n", "  return somaErrQuad"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nA *solu\u00e7\u00e3o de m\u00ednimos quadrados (least squares solution)*  \u00e9 escolher o alfa e o beta que tornam a soma dos erros quadrados a menor poss\u00edvel. Para chegar em tais valores, podemos ver o erro como uma fun\u00e7\u00e3o de $\\alpha$, $\\beta$. Vamos iniciar com o caso normalizado. Aqui $\\alpha=0$.<br>\n", "### Dados Normalizados<br>\n", "A z-normaliza\u00e7\u00e3o de dados $(x_i, y_i)$ \u00e9 obtida atr\u00e1ves de $x'_i = \\frac{x_i - \\bar{x}}{s_x}$ e $y'_i = \\frac{y_i - \\bar{y}}{s_y}$.<br>\n", "Note como a m\u00e9dia dos dados z-normalizados tanto em X quanto em Y \u00e9 zero:<br>\n", "$$\\bar{x'} = n^{-1} \\sum_{i=1}^n \\frac{x_i - \\bar{x}}{s_x}$$<br>\n", "$$\\bar{x'} = n^{-1} s^{-1}_x \\left(\\sum_{i=1}^n x_i - \\bar{x} \\sum_{i=1}^n 1\\right)$$<br>\n", "$$\\bar{x'} = n^{-1} s^{-1}_x \\left(\\sum_{i=1}^n x_i - n \\bar{x}\\right)$$<br>\n", "$$\\bar{x'} = n^{-1} s^{-1}_x \\left(\\sum_{i=1}^n x_i - n\\frac{1}{n}\\sum_{i=1}^n x_i\\right)$$<br>\n", "$$\\bar{x'} = n^{-1} s^{-1}_x \\sum_{i=1}^n \\left(x_i - x_i\\right)$$<br>\n", "$$\\bar{x'} = n^{-1} s^{-1}_x \\cdot 0 = 0$$<br>\n", "Al\u00e9m do mais, o desvio padr\u00e3o \u00e9 1. Provando:<br>\n", "$$s_{x'} = n^{-1} \\sum_i \\left(\\frac{x_i - \\bar{x}}{s_x} - 0\\right)^2$$<br>\n", "$$s_{x'} = s^{-1}_x n^{-1} \\sum_i (x_i - \\bar{x})^2$$<br>\n", "$$s_{x'} = s^{-1}_x s_x = 1$$<br>\n", "Como a m\u00e9dia dos pontos \u00e9 $(0, 0)$, a nossa melhor reta vai passar pela origem. Ou seja, $\\alpha=0$. Al\u00e9m do mais,  cada ponto dos seus dados \u00e9 uma reta entre $(0, 0)$ e o ponto $(x'_i, y'_i)$. Dessa forma, podemos resolver o problema abaixo para chegar na equa\u00e7\u00e3o da reta:<br>\n", "$$L(\\beta) = \\sum_i (y'_i - \\hat{y}'_i)^2$$<br>\n", "$$L(\\beta) = \\sum_i (y'_i - \\beta x'_i)^2$$<br>\n", "$$L(\\beta) = \\sum_i (y'^2_i - 2 \\beta x'_i y'_i + \\beta^2 x'^2_i)$$<br>\n", "Derivando a fun\u00e7\u00e3o em $\\beta$:<br>\n", "$$\\frac{d}{d\\beta} L(\\beta) = -\\sum_i (2 x'_i y'_i + 2 \\beta x'^2_i)$$<br>\n", "Fazendo $\\frac{dL}{d\\beta}=0$:<br>\n", "$$\\beta = \\frac{\\sum_i x'_i y'_i}{\\sum_i x'^2_i}$$<br>\n", "Nos slides discutimos como tal solu\u00e7\u00e3o \u00e9 uma fun\u00e7\u00e3o da covari\u00e2ncia dos dados.<br>\n", "### Dados Originais<br>\n", "Agora vamos resolver sem normalizar os dados. Vamos definir $\\Theta = [\\alpha, \\beta]$, isto \u00e9, um vetor com alfa e beta.<br>\n", "$$L(\\Theta) = \\sum_i (y_i - \\hat{y}_i)^2$$<br>\n", "$$L(\\Theta) = \\sum_i \\Big(y_i - (\\beta x_i + \\alpha)\\Big)^2$$<br>\n", "Resolvendo o quadrado dentro do somat\u00f3rio temos:<br>\n", "$$L(\\Theta) = \\sum_i (y_i^2 - 2 \\beta x_i y_i - 2 \\alpha y_i + x_i^2\\beta^2 + 2 \\beta \\alpha x_i + \\alpha^2)$$<br>\n", "Derivando em rela\u00e7\u00e3o a $\\alpha$:<br>\n", "$$\\frac{d}{d\\alpha} L(\\Theta)= \\sum_i (- 2 y_i + 2 \\beta x_i + 2\\alpha)$$<br>\n", "Derivando em rela\u00e7\u00e3o a $\\beta$:<br>\n", "$$\\frac{d}{d\\beta} L(\\Theta) = \\sum_i (- 2 x_i y_i + 2x_i^2\\beta + 2 \\alpha x_i)$$<br>\n", "Para otimizar esta fun\u00e7\u00e3o precisamos que as duas derivadas sejam zero.<br>\n", "$$\\frac{d}{d\\alpha} L(\\Theta) = 0$$<br>\n", "$$\\frac{d}{d\\beta} L(\\Theta) = 0$$<br>\n", "Ap\u00f3s isto, podemos encontrar os valores \u00f3timos de $\\alpha$ e $\\beta$. Note que esta \u00e9 uma otimiza\u00e7\u00e3o um pouco chata de resolver.<br>\n", "\\begin{align}<br>\n", " \\alpha & = \\bar{y} - \\beta\\,\\bar{x}, \\\\[5pt]<br>\n", "  \\beta &= \\frac{ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }{ \\sum_{i=1}^n (x_i - \\bar{x})^2 } \\\\[6pt]<br>\n", "            &= \\frac{ \\operatorname{Cov}(x, y) }{ \\operatorname{Var}(x) } \\\\[5pt]<br>\n", "            &= r_{xy} \\frac{s_y}{s_x}. \\\\[6pt]<br>\n", "\\end{align}<br>\n", "Sem passar pela matem\u00e1tica exata, vamos pensar em por que isso pode ser uma solu\u00e7\u00e3o razo\u00e1vel. A escolha do alfa simplesmente diz que quando vemos o valor m\u00e9dio da vari\u00e1vel independente $x$, predizemos o valor m\u00e9dio da vari\u00e1vel dependente $y$.<br>\n", "A escolha de beta significa que quando o valor de entrada aumenta pelo desvio padr\u00e3o de $x$, a previs\u00e3o de $y$ aumenta pela correla\u00e7\u00e3o entre $x$ e $y$ multiplicada pelo desvio padr\u00e3o de $y$ ($r_{xy} s_y$). No caso em que $x$ e $y$ est\u00e3o perfeitamente correlacionados, um aumento de um desvio padr\u00e3o em $x$ resulta em um aumento de um desvio padr\u00e3o de $y$ na predi\u00e7\u00e3o. Quando eles s\u00e3o perfeitamente anti-correlacionados, o aumento em $x$ resulta em uma *diminui\u00e7\u00e3o* no valor da previs\u00e3o. E quando a correla\u00e7\u00e3o \u00e9 *zero*, o beta \u00e9 *zero*, o que significa que as altera\u00e7\u00f5es em $x$ n\u00e3o afetam a previs\u00e3o.<br>\n", "Para um pouco mais sobre a intui\u00e7\u00e3o dessa f\u00f3rmula, veja este [v\u00eddeo](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/least-squares-regression/v/calculating-the-equation-of-a-regression-line).<br>\n", "- Utilizando a deriva\u00e7\u00e3o mostrada acima, implemente uma fun\u00e7\u00e3o que recebe como par\u00e2metro os valores de $x$ e $y$ e retorne os valores \u00f3timos de $\\alpha$ e $\\beta$.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def least_squares_fit(x, y):\n", "  covar = np.cov(x, y)\n", "  beta = covar[1][0] / covar[0][0]\n", "  xMedia = x.mean()\n", "  ymedia = y.mean()\n", "  alpha = ymedia - beta * xMedia\n", "  return alpha, beta"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n- A seguir, mostre o gr\u00e1fico de dispers\u00e3o dos dados e a reta gerada pelo modelo com os par\u00e2metros \u00f3timos (SEM CORRE\u00c7\u00c3O AUTOM\u00c1TICA). Quais s\u00e3o os valores \u00f3timos para $\\alpha$ e $\\beta$ ? (COM CORRE\u00c7\u00c3O AUTOM\u00c1TICA)\n<br>\n", "x = df['midparentHeight'].values<br>\n", "y = df['childHeight'].values<br>\n", "alpha, beta = least_squares_fit(x, y)<br>\n", "assert_equal(round(alpha, 2), 22.64)<br>\n", "assert_equal(round(beta, 2), 0.64)<br>\n", "plt.scatter(x, y, alpha=0.8, edgecolors='k', s=80)<br>\n", "x_model = np.linspace(64, 76, 100)<br>\n", "y_model = alpha + x_model * beta<br>\n", "plt.plot(x_model, y_model, color='magenta')<br>\n", "plt.xlabel('Altura m\u00e9dia dos pais')<br>\n", "plt.ylabel('Altura dos filhos')<br>\n", "plt.title('Regress\u00e3o linear simples')<br>\n", "despine()<br>\n", "s valores de alfa e beta encontrados nos levam a concluir que as crian\u00e7as v\u00e3o ser aproximadamente $\\alpha$ polegadas maiores do que $\\beta$ vezes a m\u00e9dia da altura dos seus pais."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Exerc\u00edcio 5: R-quadrado"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\u00c9 claro que precisamos de uma maneira melhor de descobrir o qu\u00e3o bem n\u00f3s ajustamos os dados do que simplesmente encarar o gr\u00e1fico da regress\u00e3o. Uma medida comum \u00e9 o coeficiente de determina\u00e7\u00e3o (ou R-quadrado, ou $R^2$, ou R-dois), que mede a fra\u00e7\u00e3o da varia\u00e7\u00e3o total na vari\u00e1vel dependente ($y$) que \u00e9 capturada pelo modelo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2} = 1 - \\frac{\\text{Soma dos erros quadrados}}{\\text{Soma total de quadrados}}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["* Implemente uma fun\u00e7\u00e3o que recebe como par\u00e2metro os valores de $\\alpha$, $\\beta$, $x$ e $y$ e retorne o valor do $R^2$.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def r_squared(alpha, beta, x, y):\n", "  yMedia = y.mean()\n", "  somaQuadTot = sum((y - yMedia) ** 2)\n", "  rQuad = 1 - sum_of_squared_errors(alpha, beta, x, y) / somaQuadTot\n", "  return rQuad"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nUtilizando a fun\u00e7\u00e3o implementada, qual o valor do $R^2$ no modelo com os valores \u00f3timos de $\\alpha$ e $\\beta$ ?\n<br>\n", "R2 = r_squared(alpha, beta, x, y)<br>\n", "assert_equal(round(R2, 4), 0.1030)<br>\n", " modelo linear mais simples que poder\u00edamos ter escolhido \u00e9 \"sempre prever a m\u00e9dia de $y$\" (correspondendo a `alpha = np.mean(y)` e `beta = 0`), cuja soma dos erros quadrados \u00e9 exatamente igual a sua soma total de quadrados. Tal ajuste possui um $R^2$ de zero, isto \u00e9, um modelo que (obviamente, neste caso) n\u00e3o funciona melhor do que apenas prever a m\u00e9dia."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Claramente, o modelo de m\u00ednimos quadrados deve ser pelo menos t\u00e3o bom quanto esse, o que significa que a soma dos erros quadrados \u00e9 no m\u00e1ximo a soma total de quadrados, o que significa que o $R^2$ deve ser pelo menos zero. Al\u00e9m disso, a soma dos erros quadrados deve ser pelo menos $0$, o que significa que o $R^2$ pode ser no m\u00e1ximo $1$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Quanto maior for o $R^2$, melhor o nosso modelo se ajusta aos dados. Um $R^2$ pr\u00f3ximo de 0 nos diz que o nosso modelo est\u00e1 fraco para ajustar aos dados, e que claramente existem outros fatores em jogo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Plotar os dados em outra escala pode ser uma boa forma de visualizar a variabilidade nos dados que n\u00e3o \u00e9 capturada pelo modelo.\n", "* Para visualizar isso, plote o gr\u00e1fico de dispers\u00e3o dos dados e a reta gerada pelo modelo com os par\u00e2metros \u00f3timos. Por\u00e9m, desta vez, brinque com os limites dos eixos x e y e observe a variabilidade do eixo-y que n\u00e3o \u00e9 capturada pelo modelo.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = df['midparentHeight'].values\n", "y = df['childHeight'].values\n", "alpha, beta = least_squares_fit(x, y)\n", "y_model = beta * x + alpha\n", "plt.scatter(x, y, alpha=0.8, edgecolors='k', s=80)\n", "plt.plot(x, y_model, color='magenta')\n", "plt.xlabel('Altura m\u00e9dia dos pais')\n", "plt.ylabel('Altura dos filhos')\n", "plt.title('Regress\u00e3o linear simples')\n", "plt.xlim((55, 80))\n", "plt.ylim((55, 80))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n* Por fim, verifique que a raiz de $R^2$ \u00e9 a pr\u00f3pria correla\u00e7\u00e3o.\n<br>\n", "print(r_squared(alpha, beta, x, y) ** 0.5, ss.pearsonr(x, y)[0])<br>\n", "xistem algumas formas de interpretar o $R^2$:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["1. Qu\u00e3o melhor \u00e9 o meu modelo quando comparado a um que retorna apenas a m\u00e9dia de y.\n", "1. Quanto da variancia de y \u00e9 explicada por x."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Para entender o segundo ponto, note que:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\\begin{align*}\n", "  R^2\n", "    &=  1 - \\frac{\\sum_i(y_i - \\hat{y}_i)^2}{\\sum_i(y_i - \\bar{y})^2} \\\\\n", "    &=  1 - \\frac{\\frac{1}{n-1}\\sum_i(y_i - \\hat{y}_i)^2}{\\frac{1}{n-1}\\sum_i(y_i - \\bar{y})^2} \\\\\n", "    &= 1 - \\frac{\\text{Vari\u00e2ncia Inexplicada pelo Modelo}}{\\text{Vari\u00e2ncia Total}} \\\\\n", "    &= \\frac{\\text{Vari\u00e2ncia Explicada pelo Modelo}}{\\text{Vari\u00e2ncia Total}} \\\\\n", "\\end{align*}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}