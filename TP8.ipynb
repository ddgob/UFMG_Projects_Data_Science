{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"Lista08.ipynb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Automatically generated by Colaboratory."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Original file is located at\n", "    https://colab.research.google.com/drive/1gs92tf3LJsgLOr0hHgjY3OhuhgX45yzf"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lista 8 - Verossimilhan\u00e7a"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Introdu\u00e7\u00e3o"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "# -*- coding: utf8<br>\n", "from scipy import stats as ss<br>\n", "import matplotlib.pyplot as plt<br>\n", "from numpy.testing import assert_equal<br>\n", "import numpy as np<br>\n", "import pandas as pd<br>\n", "plt.ion()<br>\n", "import seaborn as sns<br>\n", "PARAMETROS_RC = {  # Par\u00e2metros RC para o matplotlib<br>\n", "  'figure.figsize': (12, 8),<br>\n", "  'axes.labelsize': 20,<br>\n", "  'axes.titlesize': 20,<br>\n", "  'legend.fontsize': 20,<br>\n", "  'xtick.labelsize': 20,<br>\n", "  'ytick.labelsize': 20,<br>\n", "  'lines.linewidth': 4,<br>\n", "}<br>\n", "sns.set(<br>\n", "    style=\"white\",<br>\n", "    palette=\"colorblind\",<br>\n", "    rc=PARAMETROS_RC,<br>\n", ")<br>\n", "def despine(ax=None):<br>\n", "    if ax is None:<br>\n", "        ax = plt.gca()<br>\n", "    # Hide the right and top spines<br>\n", "    ax.spines['right'].set_visible(False)<br>\n", "    ax.spines['top'].set_visible(False)<br>\n", "    # Only show ticks on the left and bottom spines<br>\n", "    ax.yaxis.set_ticks_position('left')<br>\n", "    ax.xaxis.set_ticks_position('bottom')<br>\n", "ontinuando da aula passada. Vamos ver mais uma forma de entender um modelo de regress\u00e3o linear. Lembre-se at\u00e9 agora falamos de correla\u00e7\u00e3o e covari\u00e2ncia cobrindo os seguintes t\u00f3picos:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["1. Covari\u00e2ncia\n", "1. Coeficiente de Pearson (Covari\u00e2ncia Normalizada)\n", "1. Coeficiente de Pearson como sendo a fra\u00e7\u00e3o do desvio de y capturado por x\n", "1. M\u00ednimos Quadrados"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Todos os passos acima chegam no mesmo local de tra\u00e7ar a \"melhor\" reta no gr\u00e1fico de dispers\u00e3o. Melhor aqui significa a reta que que minimiza o erro abaixo:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$\\Theta = [\\alpha, \\beta]$$\n", "$$L(\\Theta) = \\sum_i (y_i - \\hat{y}_i)^2$$\n", "$$L(\\Theta) = \\sum_i (y_i - (\\beta x_i + \\alpha))^2$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["A solu\u00e7\u00e3o que minimiza tal erro \u00e9:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\\begin{align}\n", " \\alpha & = \\bar{y} - \\beta\\,\\bar{x}, \\\\[5pt]\n", "  \\beta &= \\frac{ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }{ \\sum_{i=1}^n (x_i - \\bar{x})^2 } \\\\[6pt]\n", "            &= \\frac{ \\operatorname{Cov}(x, y) }{ \\operatorname{Var}(x) } \\\\[5pt]\n", "            &= r_{xy} \\frac{s_y}{s_x}. \\\\[6pt]\n", "\\end{align}"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Vis\u00e3o probab\u00edlistica"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Vamos aprender uma \u00faltima forma de pensar na regress\u00e3o. Em particular, vamos fazer uso de uma vis\u00e3o probab\u00edlistica. Para tal, exploraremos o caso dos apartamentos de BH abaixo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Inicialmente, vamos observar os dados al\u00e9m do resultado da melhor regress\u00e3o.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/aptosBH.txt', index_col=0)\n", "df['preco'] = df['preco'] / 1000"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.scatter(df['area'], df['preco'], edgecolors='k', s=80, alpha=0.6)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.title('Pre\u00e7o de Apartamentos em BH')\n", "plt.ylabel(r'Pre\u00e7o * $10^3$ (R\\$)')\n", "plt.xlabel(r'\u00c1rea ($M^2$)')\n", "despine()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nO seaborn tem uma fun\u00e7\u00e3o regplot que plota a melhor reta al\u00e9m de um intervalo de confian\u00e7a.\n<br>\n", "sns.regplot(x='area', y='preco', data=df, n_boot=10000,<br>\n", "            line_kws={'color':'magenta', 'lw':4},<br>\n", "            scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})<br>\n", "plt.title('Preco de Apartamentos em BH')<br>\n", "plt.ylabel(r'Pre\u00e7o * $10^3$ (R\\$)')<br>\n", "plt.xlabel(r'\u00c1rea ($M^2$)')<br>\n", "despine()<br>\n", " reta pode ser recuperada usando scipy.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = ss.linregress(df['area'], df['preco'])\n", "model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nUsando esta reta podemos prever o pre\u00e7o de um apartamento usando apenas a \u00e1rea do mesmo.\n<br>\n", "beta = model.slope<br>\n", "alpha = model.intercept<br>\n", "novo_apt_area = 225<br>\n", "preco = beta * novo_apt_area + alpha<br>\n", "preco<br>\n", "u seja, quando um apartamento de 225$m^2$ entra no mercado o mesmo custa em torno de 1M de reais."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Erros Normais"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Agora, ser\u00e1 que conseguimos chegar no mesmo pensando na regress\u00e3o como um modelo probabil\u00edstico?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Ao inv\u00e9s de focar no valor quadr\u00e1tico do erro, vamos pensar no caso ideal. Qual seria o valor ideal do erro da minha regress\u00e3o? ZERO! Por\u00e9m, isso \u00e9 poss\u00edvel somente quando temos rela\u00e7\u00f5es lineares perfeitas (ex: x = altura em cm, y = altura em polegadas), o que n\u00e3o \u00e9 o caso geral."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Nessa situa\u00e7\u00e3o, vamos assumir que:\n", "$$\\epsilon_i \\sim Normal(0, \\sigma^2)$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Isto \u00e9, cada um dos erros vem de uma distribui\u00e7\u00e3o normal com m\u00e9dia $0$. Como consequ\u00eancia, a m\u00e9dia dos erros ainda vai ser uma normal centrada em $0$; e o desvio padr\u00e3o dos erros poder\u00e1 ser estimados atrav\u00e9s dos dados ap\u00f3s a regress\u00e3o."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Verossimilhan\u00e7a"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Na regress\u00e3o, queremos prever o valor de $y$ em fun\u00e7\u00e3o de $x$. No exemplo mostrado acima, queremos prever o pre\u00e7o de compra de um apartamento em BH em fun\u00e7\u00e3o da \u00e1rea."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Estamos assumindo que:\n", "$$\\epsilon_i = y_i - \\beta x_i - \\alpha$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Ou seja, um erro na estimativa de um ponto n\u00e3o depende de outros."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Temos uma fun\u00e7\u00e3o de densidade de probabilidade para os erros:\n", "$$\\epsilon_i \\sim Normal(0, \\sigma^2)$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Observe a fun\u00e7\u00e3o de densidade da Normal:\n", "$$p(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Logo:\n", "$$p(\\epsilon_i|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(\\epsilon_i-\\mu)^2}{2 \\sigma^2}}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Como no nosso modelo a m\u00e9dia \u00e9 zero, ficamos com:\n", "$$p(\\epsilon_i|\\mu=0, \\sigma^2) = \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{\\epsilon_i^2}{2 \\sigma^2}}$$\n", "$$p(\\epsilon_i|\\sigma^2, \\alpha, \\beta) = \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["A probabilidade mostrada acima \u00e9 a probabilidade de uma observa\u00e7\u00e3o (observar o erro $\\epsilon_i$) dado os par\u00e2metros $\\sigma^2$, $\\alpha$ e $\\beta$!  \n", "Isto \u00e9 uma **verossimilhan\u00e7a**!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["O gr\u00e1fico abaixo mostra a distribui\u00e7\u00e3o de probabilidade para erros vindos de uma distribui\u00e7\u00e3o normal com m\u00e9dia igual a zero e desvio padr\u00e3o igual a um.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = np.linspace(-5, 5, 100)\n", "plt.plot(x, ss.distributions.norm.pdf(x, scale=1))\n", "plt.xlabel(r'$\\epsilon_i$')\n", "plt.ylabel(r'$p(\\epsilon_i \\mid \\mu=0, \\sigma=1)$')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["despine()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nUm modelo estat\u00edstico nada mais \u00e9 do que um conjunto de hip\u00f3teses que s\u00e3o supostas como v\u00e1lidas para a distribui\u00e7\u00e3o de probabilidades das vari\u00e1veis aleat\u00f3rias medidas na amostra. Em particular, estamos falando de modelos param\u00e9tricos, onde assumimos que o erro \u00e9 uma normal com uma fun\u00e7\u00e3o de densidade conhecida (como mostrado no gr\u00e1fico acima).<br>\n", "Vamos focar numa base de 3 pontos. Al\u00e9m do mais, vamos assumir uma reta qualquer, dessa forma, o desvio padr\u00e3o dos erros podem ser estimados atrav\u00e9s dos dados ap\u00f3s a regress\u00e3o. Se os erros v\u00eam de uma distribui\u00e7\u00e3o normal com fun\u00e7\u00e3o de densidade j\u00e1 conhecida, qual dos tr\u00eas erros representados na figura abaixo \u00e9 o mais veross\u00edmil?<br>\n", "Observe:<br>\n", "$$p(\\epsilon_i|\\sigma^2, \\alpha, \\beta) = \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}$$<br>\n", "Quanto menor o erro, maior a probabilidade, sendo assim, o segundo erro \u00e9 o mais veross\u00edmil.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["beta = 1\n", "alpha = 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = plt.figure(figsize=(36, 10))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = np.array([2, 8, 5])\n", "y = np.array([0, 1, 3])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(121)\n", "plt.scatter(x, y, edgecolors='k', s=80, alpha=0.6)\n", "plt.title('3 Pontinhos')\n", "plt.ylabel(r'Y')\n", "plt.xlabel(r'X')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_bar = x * beta + alpha\n", "plt.plot(x, y_bar, color='magenta')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_min = [min(y_i, y_bar_i) for y_i, y_bar_i in zip(y, y_bar)]\n", "y_max = [max(y_i, y_bar_i) for y_i, y_bar_i in zip(y, y_bar)]\n", "plt.vlines(x, ymin=y_min, ymax=y_max, color='magenta', lw=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["despine()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(122)\n", "plt.title('PDF da Normal')\n", "ei_x = np.linspace(-10, 10, 100)\n", "sigma = (y - y_bar).std(ddof=1)\n", "plt.plot(ei_x, ss.distributions.norm.pdf(ei_x, scale=sigma))\n", "plt.xlabel(r'$\\epsilon_i$')\n", "plt.ylabel(r'$p(\\epsilon_i \\mid \\mu=0, \\sigma={})$'.format(np.round(sigma, 2)))\n", "despine()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nObserve o exemplo considerando agora todos os pontos do exemplo de predi\u00e7\u00e3o de pre\u00e7o de apartamentos em BH.\n<br>\n", "beta = 3.535719156333653<br>\n", "alpha = 200.52361368989432<br>\n", "fig = plt.figure(figsize=(36, 10))<br>\n", "x = df['area']<br>\n", "y = df['preco']<br>\n", "plt.subplot(121)<br>\n", "plt.scatter(x, y, edgecolors='k', s=80, alpha=0.6)<br>\n", "plt.title('Preco de Apartamentos em BH')<br>\n", "plt.ylabel(r'Pre\u00e7o * $10^3$ (R\\$)')<br>\n", "plt.xlabel(r'\u00c1rea ($M^2$)')<br>\n", "y_bar = x * beta + alpha<br>\n", "plt.plot(x, y_bar, color='magenta')<br>\n", "y_min = [min(y_i, y_bar_i) for y_i, y_bar_i in zip(y, y_bar)]<br>\n", "y_max = [max(y_i, y_bar_i) for y_i, y_bar_i in zip(y, y_bar)]<br>\n", "plt.vlines(x, ymin=y_min, ymax=y_max, color='magenta', lw=1)<br>\n", "despine()<br>\n", "plt.subplot(122)<br>\n", "plt.title('PDF da Normal')<br>\n", "ei_x = np.linspace(-1000, 1000, 100)<br>\n", "sigma = (y - y_bar).std(ddof=1)<br>\n", "plt.plot(ei_x, ss.distributions.norm.pdf(ei_x, scale=sigma))<br>\n", "plt.xlabel(r'$\\epsilon_i$')<br>\n", "plt.ylabel(r'$p(\\epsilon_i \\mid \\mu=0, \\sigma={})$'.format(np.round(sigma, 2)))<br>\n", "despine()<br>\n", "# Erros Independentes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Nos exemplos acima discutimos sobre qual erro \u00e9 o mais veross\u00edmil... Mas como podemos calcular a verossimilhan\u00e7a de todos os erros?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Ao assumir **independ\u00eancia**, a probabilidade total \u00e9 obtida diretamente do produto das probabilidades. Logo:\n", "$$p(E|\\sigma^2, \\alpha, \\beta) = \\prod_i p(\\epsilon_i|\\sigma^2, \\alpha, \\beta) = \\prod_i p(\\epsilon_i|\\theta)$$\n", "onde $E = \\{e_1, e_2, ..., e_n\\}$ e $\\theta = \\{\\sigma^2, \\alpha, \\beta\\}$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Acabamos de definir nossa primeira fun\u00e7\u00e3o de verossimilhan\u00e7a. Como podemos proceder agora? Maximizando a mesma! Ou seja, queremos os par\u00e2metros ($\\theta$) que melhor se ajustam os nossos dados.\n", "Por\u00e9m, maximizar produt\u00f3rios \u00e9 chato... O mundo das somas \u00e9 mais bacana do que o mundo da multiplica\u00e7\u00e3o."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Log-verossimilhan\u00e7a"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Como o $\\log$ \u00e9 uma fun\u00e7\u00e3o monotonicamente crescente, maximizar o $\\log(f(x))$ \u00e9 equivalente a maximizar $f(x)$. Portanto, podemos trabalhar apenas com o $\\log$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Ao inv\u00e9s de maximizar a fun\u00e7\u00e3o de **verossimilhan\u00e7a** $L(\\theta)$, vamos maximizar a fun\u00e7\u00e3o de **log-verossimilhan\u00e7a** $l(\\theta)$:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$L(\\theta) = \\prod_i p(\\epsilon_i|\\theta)$$\n", "$$\\log(L(E|\\theta)) = \\log\\left(\\prod_i p(\\epsilon_i|\\theta)\\right)$$\n", "$$l(\\epsilon_i|\\theta) = \\sum_i \\log(p(\\epsilon_i|\\theta))$$\n", "uma vez que $\\log(AB) = \\log(A) + \\log(B)$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Substituindo os par\u00e2metros $\\sigma^2$, $\\alpha$, $\\beta$ nas fun\u00e7\u00f5es e fazendo algumas manipula\u00e7\u00f5es, temos que a verossimilhan\u00e7a e a log-verossimilhan\u00e7a s\u00e3o dadas por:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$L(E|\\theta) = \\prod_i \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$l(E|\\theta) = -n\\log(\\sqrt{2\\pi}) -n\\log(\\sigma) - \\frac{\\sum_i (y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["* A partir da f\u00f3rmula da verossimilhan\u00e7a, dada por:\n", "$$L(E|\\theta) = \\prod_i \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}$$\n", "verifique que a express\u00e3o da log-verossimilhan\u00e7a \u00e9 dada por:\n", "$$l(E|\\theta) = -n\\log(\\sqrt{2\\pi}) -n\\log(\\sigma) - \\frac{\\sum_i (y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$L(E|\\theta) = \\prod_i \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}$$\n", "$$\\log(L(E|\\theta)) = \\log\\left(\\prod_i \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}\\right)$$\n", "$$l(\\epsilon_i|\\theta) = \\sum_i \\log\\left(\\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}\\right)$$\n", "uma vez que $log(AB) = log(A) + log(B)$.\n", "$$l(\\epsilon_i|\\theta) = \\sum_i \\log\\left(\\frac{1}{\\sqrt{\\sigma^2 2 \\pi}}\\right) + \\sum_i log\\left(e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}\\right)$$\n", "$$l(\\epsilon_i|\\theta) = n \\log\\left(\\frac{1}{\\sqrt{\\sigma^2 2 \\pi}}\\right) + \\sum_i \\log\\left(e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}\\right)$$\n", "$$l(\\epsilon_i|\\theta) = n \\log\\left(\\frac{1}{\\sqrt{\\sigma^2 2 \\pi}}\\right) - \\frac{\\sum_i (y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}$$\n", "uma vez que $\\log(A^x) = x \\log(A)$ e $\\log(e) = 1$.\n", "$$l(\\epsilon_i|\\theta) = - n \\log\\left(\\sqrt{\\sigma^2 2 \\pi}\\right) - \\frac{\\sum_i (y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}$$\n", "uma vez que $\\log(\\frac{1}{x}) = - \\log(x)$.\n", "$$l(\\epsilon_i|\\theta) = - n \\log(\\sqrt{2 \\pi}) - n \\log(\\sqrt{\\sigma}) - \\frac{\\sum_i (y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Maximizar a log-verossimilhan\u00e7a implica em achar os par\u00e2metros que melhor se ajustam aos dados. Al\u00e9m disso, lembre-se que maximizar \u00e9 achar o local onde a derivada \u00e9 zero."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Para isso, devemos resolver as derivadas em rela\u00e7\u00e3o a cada um dos par\u00e2metros: $\\sigma$, $\\alpha$ e $\\beta$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$\\alpha = \\bar{y} - \\beta \\bar{x}$$\n", "$$\\beta = \\frac{ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }{ \\sum_{i=1}^n (x_i - \\bar{x})^2 }$$\n", "$$\\sigma^2 = \\frac{\\sum_i (y_i - \\beta x_i - \\alpha)^2}{n}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Observe que ao maximizar $\\sigma$, chegamos no estimador da vari\u00e2ncia. Al\u00e9m disso, o $\\sigma$ n\u00e3o impacta os valores de $\\alpha$ e $\\beta$, mas \u00e9 \u00fatil para entendermos os erros do estimador (\u00e9 a vari\u00e2ncia dos erros afinal)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Por fim, note que maximizar a verossimilhan\u00e7a da regress\u00e3o \u00e9 o mesmo que minimizar os erros quadrados."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Estima\u00e7\u00e3o da M\u00e1xima Verossimilhan\u00e7a e M\u00ednimos Quadrados"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Por que escolher os m\u00ednimos quadrados? Uma justificativa envolve a estimativa de m\u00e1xima verossimilhan\u00e7a."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Imagine que temos uma amostra de dados $v_1, \\cdots, v_n$ que vem de uma distribui\u00e7\u00e3o que depende de algum par\u00e2metro desconhecido $\\theta$:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$p(v_1, \\cdots, v_n~|~\\theta)$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Se n\u00e3o conhec\u00eassemos theta, poder\u00edamos nos virar e pensar nessa quantidade como a probabilidade de $\\theta$ dada a amostra:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$L(\\theta~|~v_1, \\cdots, v_n)$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Sob essa abordagem, o mais prov\u00e1vel $\\theta$ \u00e9 o valor que maximiza essa fun\u00e7\u00e3o de verossimilhan\u00e7a; isto \u00e9, o valor que torna os dados observados os mais prov\u00e1veis. Podemos fazer a mesma coisa no caso de uma distribui\u00e7\u00e3o cont\u00ednua, na qual temos uma fun\u00e7\u00e3o de distribui\u00e7\u00e3o de probabilidade e n\u00e3o uma fun\u00e7\u00e3o de massa de probabilidade."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["De volta \u00e0 regress\u00e3o. Uma suposi\u00e7\u00e3o que muitas vezes \u00e9 feita sobre o modelo de regress\u00e3o simples \u00e9 que os erros de regress\u00e3o s\u00e3o normalmente distribu\u00eddos com m\u00e9dia $0$ e algum desvio padr\u00e3o (conhecido) $\\sigma$. Se esse for o caso, a probabilidade de ver um par $(x_i, y_i)$ \u00e9:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$L(\\alpha, \\beta~|~x_i, y_i, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp{\\left(\\frac{-(y_i-\\alpha-\\beta x_i)^2}{2\\sigma^2}\\right)}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["A probabilidade de todo o conjunto de dados \u00e9 o produto das probabilidades individuais, que \u00e9 maior precisamente quando alfa e beta s\u00e3o escolhidos para minimizar a soma dos erros quadrados. Ou seja, nesse caso (e com essas suposi\u00e7\u00f5es), minimizar a soma dos erros quadrados \u00e9 equivalente a maximizar a probabilidade dos dados observados."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Qualidade da regress\u00e3o"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Ap\u00f3s encontrar os par\u00e2metros do modelo que melhor se ajustam aos dados (minimizam os erros quadrados, maximizam a verossimilhan\u00e7a), devemos avaliar a qualidade do ajuste. A qualidade da regress\u00e3o pode ser analisada atrav\u00e9s de gr\u00e1ficos ou m\u00e9tricas, como o R-quadrado, estudado anteriormente."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Gr\u00e1fico residual"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["O gr\u00e1fico residual \u00e9 uma forma de visualizar os erros versus a vari\u00e1vel preditora para verificar a pressuposi\u00e7\u00e3o de que os erros s\u00e3o aleatoriamente distribu\u00eddos e t\u00eam vari\u00e2ncia constante. De maneira ideal, os pontos devem cair aleatoriamente em ambos os lados de 0, sem padr\u00f5es reconhec\u00edveis nos pontos. Como os erros s\u00e3o centrados em zero, ao plotar valor de x pelo erro queremos pontos igualmente dispersados  positivos e negativos.\n", "Lembre-se da PDF da distribui\u00e7\u00e3o normal. A mediana e a m\u00e9dia s\u00e3o iguais, ou seja, 50% dos erros se concentram acima de 0 e 50% abaixo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Observe o gr\u00e1fico residual para o exemplo de prever o pre\u00e7o de um apartamento usando apenas a \u00e1rea do mesmo. No eixo-x temos a \u00e1rea do aparatamento e no eixo-y o erro. O gr\u00e1fico n\u00e3o parece ter um padr\u00e3o (\u00e9 o que queremos) j\u00e1 que os erros est\u00e3o espalhados em ambos os lados da linha tracejada que marca o erro igual a 0. Entretanto, os erros aparentam ser piores quando a \u00e1rea dos apartamentos (vari\u00e1vel x) aumenta.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.residplot(x='area', y='preco', data=df,\n", "              line_kws={'color':'magenta', 'lw':4},\n", "              scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})\n", "plt.ylabel(r'$\\epsilon_i$')\n", "plt.xlabel(r'\u00c1rea ($M^2$)')\n", "despine()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## QQ Plot<br>\n", "Por fim, outra forma de verificar se o modelo \u00e9 bom se chama QQ Plot.<br>\n", "Esse gr\u00e1fico \u00e9 \u00fatil para checar a adequa\u00e7\u00e3o da distribui\u00e7\u00e3o de frequ\u00eancia dos dados \u00e0 uma distribui\u00e7\u00e3o de probabilidades. No caso dos modelos de regress\u00e3o, o QQ Plot \u00e9 usado para verificar se os erros apresentam distribui\u00e7\u00e3o normal.<br>\n", "Uma forma de construir o QQ Plot consiste em ordenar os erros (eixo-y), comparando com o local esperado do mesmo no modelo (distribui\u00e7\u00e3o normal). Nesse caso, a mediana fica no centro do plot. O erro mediano \u00e9 zero caso a normal seja verdadeira!<br>\n", "Outra forma de construir o QQ Plot consiste em pegar o valor z-norm que leva para a probabilidade de cada um dos erros ordenados no modelo e plotar os erros ordenados no eixo-y e os valores z-normalizados no eixo-x. O gr\u00e1fico abaixo apresenta o QQ Plot do exemplo de prever o pre\u00e7o de um apartamento usando apenas a \u00e1rea do mesmo.<br>\n", "Idealmente quanto mais pr\u00f3ximo de uma reta melhor, uma vez que a reta quer dizer que os erros s\u00e3o perfeitamente normais. Sendo assim, na regress\u00e3o perfeita observamos uma linha reta no QQ plot.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ss.probplot(y - y_bar, plot=plt.gca());"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Exerc\u00edcios - Outros datasets<br>\n", "### Enunciado<br>\n", "Abaixo est\u00e3o listados algumas conjuntos de dados contendo rela\u00e7\u00f5es entre vari\u00e1veis:<br>\n", "**Supernovas**: Atualmente, uma das teorias mais aceitas sobre a forma\u00e7\u00e3o do universo, diz que o universo est\u00e1 em constante expans\u00e3o. Supernovas s\u00e3o estrelas que explodiram e morreram recentemente. Os dados cont\u00eam registros dessas supernovas. Cada linha na tabela corresponde a uma supernova pr\u00f3xima da Terra observada por astr\u00f4nomos, indicando o qu\u00e3o longe da Terra a supernova estava e o qu\u00e3o r\u00e1pido ela se afastava.<br>\n", "**Stocks and Unemployment**: A Bolsa est\u00e1 diretamente ligada \u00e0 economia do pa\u00eds e do mundo. Quando ela desaba, isso pode ter consequ\u00eancias negativas at\u00e9 no dia a dia de quem nem sabe como a Bolsa funciona. Por exemplo, o desemprego pode aumentar e a infla\u00e7\u00e3o acelerar, tornando os produtos no supermercado mais caros. Com base nisso, os dados apresentam a evolu\u00e7\u00e3o no pre\u00e7o de uma a\u00e7\u00e3o e a taxa de desemprego em determinado per\u00edodo.<br>\n", "**Stocks and Interest**: Al\u00e9m de relacionar o valor de a\u00e7\u00f5es na Bolsa com o \u00edndice de desemprego, \u00e9 poss\u00edvel correlacionar o valor das a\u00e7\u00f5es e outros indicadores. Os dados apresentam a evolu\u00e7\u00e3o no pre\u00e7o de uma a\u00e7\u00e3o e a taxa de juros em determinado per\u00edodo.<br>\n", "**Dugongs**: Os dados incluem informa\u00e7\u00f5es de peixe-bois. Cada linha cont\u00e9m o comprimento e a idade de indiv\u00edduos. A base de dados \u00e9 muito utilizada para fazer predi\u00e7\u00f5es do comprimento desses animais de acordo com a idade.<br>\n", "- **Exerc\u00edcio:** Para cada um dos datasets voc\u00ea dever\u00e1:<br>\n", "    1. Carregar os dados do arquivo.  <br>\n", "    2. Fazer uma regress\u00e3o linear.  <br>\n", "    3. Avaliar a qualidade do modelo atrav\u00e9s do c\u00e1lculo do $R^2$ e dos gr\u00e1ficos de erros: Gr\u00e1fico residual e QQ Plot.  <br>\n", "    4. Verificar se os erros s\u00e3o independentes. Para isso, conte quantos erros s\u00e3o maiores e quantos s\u00e3o menores do que zero.<br>\n", "Antes de realizar o exerc\u00edcio, \u00e9 \u00fatil voc\u00ea definir as seguintes fun\u00e7\u00f5es com base nos par\u00e2metros citados em cada uma:<br>\n", "Note que ap\u00f3s definir as fun\u00e7\u00f5es pedidas corretamente at\u00e9 o fim do primeiro dataset, os demais ir\u00e3o executar sem erros, n\u00e3o precisando ser alterados.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def error(alpha, beta, x, y):\n", "    return y - (beta * x + alpha)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sum_of_squared_errors(alpha, beta, x, y):\n", "    return sum(error(alpha, beta, x, y) ** 2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def total_sum_of_squares(y):\n", "    yMean = y.mean()\n", "    return sum((y - yMean) ** 2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def r_squared(alpha, beta, x, y):\n", "    sse = sum_of_squared_errors(alpha, beta, x, y)\n", "    Tsse = total_sum_of_squares(y)\n", "    return 1 - sse / Tsse"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test(alpha, beta, x, y):\n", "    sq_res = sum((y - (beta * x + alpha)) ** 2)\n", "    y_mean = y.mean()\n", "    sq_tot = sum((y - y_mean) ** 2)\n", "    return 1 - sq_res / sq_tot"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n### 1. Supernovas<br>\n", "#### 1.1 Carregar os dados<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/close_novas.csv')\n", "x = df['Distance (million parsecs)']\n", "y = df['Speed (parsecs/year)']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n#### 1.2 Regress\u00e3o Linear\n<br>\n", "def linear_regression(x, y):<br>\n", "    xMean = x.mean()<br>\n", "    yMean = y.mean()<br>\n", "    cov = np.cov(x, y)<br>\n", "    beta = cov[1][0] / cov[0][0]<br>\n", "    alpha = yMean - beta * xMean<br>\n", "    return alpha, beta<br>\n", "(a1, b1) = linear_regression(x,y)<br>\n", "assert_equal(round(a1, 6), 0.000167)<br>\n", "assert_equal(round(b1, 6), 0.000068)<br>\n", "### 1.3 Avalia\u00e7\u00e3o do Modelo via R\u00b2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "# avaliacao do Rsquared<br>\n", "r2 = r_squared(a1, b1, x, y)<br>\n", "assert_equal(round(r2, 4), 0.9645)<br>\n", "sns.residplot(x='Distance (million parsecs)', y='Speed (parsecs/year)', data=df, line_kws={'color':'magenta', 'lw':4}, scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})<br>\n", "plt.ylabel(r'$\\epsilon_i$')<br>\n", "despine()<br>\n", "plt.show()<br>\n", "# GABARITO GRAFICO<br>\n", "# sns.residplot(x='Distance (million parsecs)', y='Speed (parsecs/year)', data=df,<br>\n", "#               line_kws={'color':'magenta', 'lw':4},<br>\n", "#               scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})<br>\n", "# plt.ylabel(r'$\\epsilon_i$')<br>\n", "# despine()<br>\n", "# plt.show()<br>\n", "### 1.4 Verificando se os erros s\u00e3o independentes\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def error_verification(alpha, beta, x, y):\n", "    neg: int = 0\n", "    pos: int = 0\n", "    for (i, j) in zip(x, y):\n", "        if error(alpha, beta, i, j) < 0:\n", "            neg = neg + 1\n", "        else:\n", "            pos = pos + 1\n", "    return (pos, neg)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["(maiores, menores) = error_verification(a1, b1, x, y)\n", "assert_equal(maiores, 78)\n", "assert_equal(menores, 78)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ss.probplot(y - y.mean(), plot=plt)\n", "despine()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ss.probplot(y - y.mean(), plot=plt)<br>\n", "despine()<br>\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n### 2. Stocks anad Unemployment<br>\n", "#### 2.1 Carregar os dados<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/stocks_unemployment.csv')\n", "x = df['Unemployment_Rate']\n", "y = df['Stock_Index_Price']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n#### 2.2 Regress\u00e3o Linear\n<br>\n", "(a2, b2) = linear_regression(x,y)<br>\n", "assert_equal(round(a2, 6), 4471.339321)<br>\n", "assert_equal(round(b2, 6), -588.962076)<br>\n", "### 2.3 Avalia\u00e7\u00e3o do Modelo via R\u00b2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "# avaliacao do Rsquared<br>\n", "r2 = r_squared(a2, b2, x, y)<br>\n", "assert_equal(round(r2, 4), 0.8507)<br>\n", "sns.residplot(x='Unemployment_Rate', y='Stock_Index_Price', data=df, line_kws={'color':'magenta', 'lw':4}, scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})<br>\n", "plt.ylabel(r'$\\epsilon_i$')<br>\n", "despine()<br>\n", "plt.show()<br>\n", "# GABARITO GRAFICO<br>\n", "# sns.residplot(x='Unemployment_Rate', y='Stock_Index_Price', data=df,<br>\n", "#               line_kws={'color':'magenta', 'lw':4},<br>\n", "#               scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})<br>\n", "# plt.ylabel(r'$\\epsilon_i$')<br>\n", "# despine()<br>\n", "# plt.show()<br>\n", "### 2.4 Verificando se os erros s\u00e3o independentes\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["(maiores, menores) = error_verification(a2, b2, x, y)\n", "assert_equal(maiores, 13)\n", "assert_equal(menores, 11)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ss.probplot(y - y.mean(), plot=plt)\n", "despine()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ss.probplot(y - y.mean(), plot=plt)<br>\n", "despine()<br>\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n### 3. Stocks and Interest<br>\n", "#### 3.1 Carregar os dados<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/stocks_interest.csv')\n", "x = df['Interest_Rate']\n", "y = df['Stock_Index_Price']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n#### 3.2 Regress\u00e3o Linear\n<br>\n", "(a3, b3) = linear_regression(x,y)<br>\n", "assert_equal(round(a3, 6), -99.464319)<br>\n", "assert_equal(round(b3, 6), 564.203892)<br>\n", "### 3.3 Avalia\u00e7\u00e3o do Modelo via R\u00b2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "# avaliacao do Rsquared<br>\n", "r2 = r_squared(a3, b3, x, y)<br>\n", "assert_equal(round(r2, 4), 0.8757)<br>\n", "sns.residplot(x='Interest_Rate', y='Stock_Index_Price', data=df, line_kws={'color':'magenta', 'lw':4}, scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})<br>\n", "plt.ylabel(r'$\\epsilon_i$')<br>\n", "despine()<br>\n", "plt.show()<br>\n", "# GABARITO GRAFICO<br>\n", "# sns.residplot(x='Interest_Rate', y='Stock_Index_Price', data=df,<br>\n", "#               line_kws={'color':'magenta', 'lw':4},<br>\n", "#               scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})<br>\n", "# plt.ylabel(r'$\\epsilon_i$')<br>\n", "# despine()<br>\n", "# plt.show()<br>\n", "### 3.4 Verificando se os erros s\u00e3o independentes\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["(maiores, menores) = error_verification(a3, b3, x, y)\n", "assert_equal(maiores, 12)\n", "assert_equal(menores, 12)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ss.probplot(y - y.mean(), plot=plt)\n", "despine()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ss.probplot(y - y.mean(), plot=plt)<br>\n", "despine()<br>\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n### 4. Dugongs<br>\n", "#### 4.1 Carregar os dados<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('https://raw.githubusercontent.com/pedroharaujo/ICD_Docencia/master/dugongs.csv')\n", "x = df['Age']\n", "y = df['Length']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n#### 4.2 Regress\u00e3o Linear\n<br>\n", "(a4, b4) = linear_regression(x,y)<br>\n", "assert_equal(round(a4, 6), 2.018286)<br>\n", "assert_equal(round(b4, 6), 0.028955)<br>\n", "### 4.3 Avalia\u00e7\u00e3o do Modelo via R\u00b2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "# avaliacao do Rsquared<br>\n", "r2 = r_squared(a4, b4, x, y)<br>\n", "assert_equal(round(r2, 4), 0.6883)<br>\n", "sns.residplot(x='Age', y='Length', data=df, line_kws={'color':'magenta', 'lw':4}, scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})<br>\n", "plt.ylabel(r'$\\epsilon_i$')<br>\n", "despine()<br>\n", "plt.show()<br>\n", "# GABARITO GRAFICO<br>\n", "# sns.residplot(x='Age', y='Length', data=df,<br>\n", "#               line_kws={'color':'magenta', 'lw':4},<br>\n", "#               scatter_kws={'edgecolor':'k', 's':80, 'alpha':0.8})<br>\n", "# plt.ylabel(r'$\\epsilon_i$')<br>\n", "# despine()<br>\n", "# plt.show()<br>\n", "### 4.4 Verificando se os erros s\u00e3o independentes\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["(maiores, menores) = error_verification(a4, b4, x, y)\n", "assert_equal(maiores, 16)\n", "assert_equal(menores, 11)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ss.probplot(y - y.mean(), plot=plt)\n", "despine()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ss.probplot(y - y.mean(), plot=plt)<br>\n", "despine()<br>\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## Estimador de M\u00e1xima Verossimilhan\u00e7a<br>\n", "Conforme vimos, a verossimilhan\u00e7a representa a probabilidade dos nossos dados $X$ (ou erros $E$) condicionados em um modelo. O modelo aqui s\u00e3o os par\u00e2metros $\\theta$ e a fun\u00e7\u00e3o de probabilidade $p$.<br>\n", "$$X = \\{x_1, x_2, ..., x_n\\}$$<br>\n", "$$p(X|\\theta) = \\prod_i p(x_i|\\theta)$$<br>\n", "No modelo que estudamos:<br>\n", "$\\epsilon_i \\sim Normal(0, \\sigma^2)$  <br>\n", "$p(\\epsilon_i|\\sigma^2, \\alpha, \\beta) = \\frac{1}{\\sqrt{\\sigma^2 2 \\pi}} e^{-\\frac{(y_i - \\beta x_i - \\alpha)^2}{2 \\sigma^2}}$  <br>\n", "$E = \\{e_1, e_2, ..., e_n\\}$ e $\\theta = \\{\\sigma^2, \\alpha, \\beta\\}$  <br>\n", "$p(E|\\theta) = \\prod_i p(\\epsilon_i|\\theta)$<br>\n", "Entretanto, podemos estimar os par\u00e2metros de qualquer modelo probabil\u00edstico.<br>\n", "Imagine que observamos vari\u00e1veis categ\u00f3ricas $X$. $X$ corresponde ao efeito do rem\u00e9dio em um grupo de pessoas, onde cada $x_i$ assume valo igual a 1, caso o rem\u00e9dio tenha funcionado e 0 caso contr\u00e1rio. Por exemplo: $X = \\{1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0\\}$<br>\n", "Podemos modelar estes dados a partir de uma distribui\u00e7\u00e3o [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution). Ou seja,  $x_i \\sim Bernoulli(\\theta)$ onde $\\theta$ representa a probabilidade do rem\u00e9dio funcionar para o paciente (probabilidade de sucesso).<br>\n", "* Derive o estimador de m\u00e1xima verossimilhan\u00e7a para a distribui\u00e7\u00e3o Bernoulli.<br>\n", "$$x_i \\sim \\text{Bernoulli}(\\theta)$$<br>\n", "$$p(x_i|\\theta) = \\theta^{x_i} (1-\\theta)^{(1 - x_i)}$$<br>\n", "Estimador de m\u00e1xima verossimilhan\u00e7a:<br>\n", "$$L(X|\\theta) = \\prod_i p(x_i|\\theta) = \\prod_i \\theta^{x_i} (1-\\theta)^{(1 - x_i)} = \\theta^{\\sum_i x_i} (1-\\theta)^{\\sum_i (1 - x_i)}$$<br>\n", "lembrando que $a^x b^y * a^z b^w = a^{x+z} b^{y+w}$<br>\n", "Estimador de m\u00e1xima log-verossimilhan\u00e7a:<br>\n", "$$l(X|\\theta) = \\log\\left(\\prod_i p(x_i|\\theta)\\right) = \\sum_i \\log\\left(\\theta^{x_i} (1-\\theta)^{(1 - x_i)}\\right) = \\sum_i x_i \\log(\\theta) + \\sum_i (1-x_i) \\log(1-\\theta)$$<br>\n", "lembrando que $\\log(ab) = \\log(a) + \\log(b)$ e $\\log(a^x) = x\\log(a)$<br>\n", "* Ap\u00f3s derivar o estimador de m\u00e1xima verossimilhan\u00e7a para a distribui\u00e7\u00e3o Bernoulli encontre o valor de $\\theta$ que maximiza a verossimilhan\u00e7a.<br>\n", "Assumimos que as observa\u00e7\u00f5es s\u00e3o independentes, maximizar a log da verossimilhan\u00e7a corresponde a derivar essa fun\u00e7\u00e3o e igualar a zero, encontrando o seu ponto de m\u00e1ximo.<br>\n", "Calculando a derivada, temos:<br>\n", "$$\\frac{\\partial l(X|\\theta)}{\\theta} = \\frac{\\partial (\\sum_i x_i \\log(\\theta) + \\sum_i (1-x_i) \\log(1-\\theta))}{\\partial \\theta} = \\frac{\\sum_i x_i}{\\theta} - \\frac{\\sum_i (1-x_i)}{(1-\\theta)}$$<br>\n", "Lembrando que $\\frac{\\partial log(a)}{\\partial a} = \\frac{1}{a}$ e $\\frac{\\partial log(1-a)}{\\partial a} = \\frac{1}{(1-a)}$<br>\n", "Igualando a derivada a zero, encontramos o valor de $\\theta$ que maximiza a log-verossimilhan\u00e7a:<br>\n", "$$\\frac{\\sum_i x_i}{\\theta} - \\frac{\\sum_i (1-x_i)}{(1-\\theta)} = 0$$<br>\n", "$$(1-\\theta) \\sum_i x_i = \\theta \\sum_i (1-x_i)$$<br>\n", "$$\\sum_i x_i - \\theta \\sum_i x_i = n\\theta - \\theta \\sum_i x_i$$<br>\n", "$$\\sum_i x_i = n\\theta$$<br>\n", "$$\\theta = \\frac{\\sum_i x_i}{n} = \\bar x$$<br>\n", ""]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}